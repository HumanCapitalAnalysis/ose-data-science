

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Instrumental variable estimators of causal effects &mdash; OSE data science  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="The Generalized Roy Model" href="../generalized-roy-model/notebook.html" />
    <link rel="prev" title="Self-selection, heterogeneity, and causal graphs" href="../selection-heterogeneity-graphs/notebook.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> OSE data science
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Lectures</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#potential-outcome-model">Potential outcome model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#causal-graphs">Causal graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#conditioning-estimators">Conditioning estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#matching-estimators">Matching estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#regression-estimators">Regression estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#heterogeneity-selection-and-causal-graphs">Heterogeneity, selection, and causal graphs</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#instrumental-variables">Instrumental variables</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Instrumental variable estimators of causal effects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Causal-effect-estimation-with-a-binary-IV">Causal effect estimation with a binary IV</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Demonstration-dataset">Demonstration dataset</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Traditional-IV-estimators">Traditional IV estimators</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Instrumental-variable-estimators-in-the-presence-of-individual-level-heterogeneity">Instrumental variable estimators in the presence of individual-level heterogeneity</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Criticism">Criticism</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Discussion">Discussion</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Resources">Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#generalized-roy-model">Generalized Roy model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#causal-explanations">Causal explanations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#repeated-observations">Repeated observations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#regression-discontinuity-design">Regression discontinuity design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#generalized-method-of-moments">Generalized method of moments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../problem-sets/index.html">Problem sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../handouts/index.html">Handouts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects/index.html">Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datasets/index.html">Data sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../partners/index.html">Partners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../organization/index.html">Organization</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">OSE data science</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Lectures</a> &raquo;</li>
        
      <li>Instrumental variable estimators of causal effects</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/lectures/instrumental-variable/notebook.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="admonition note">
<p>Download the notebook <a class="reference download external" download="" href="https://nbviewer.jupyter.org/github/HumanCapitalAnalysis/ose-data-science/blob/master/lecturesinstrumental-variablenotebook.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>!
Interactive online version: <a class="reference external" href="https://mybinder.org/v2/gh/HumanCapitalAnalysis/ose-data-science/master?filepath=lecturesinstrumental-variablenotebook.ipynb"><img alt="binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
</div>
<div class="section" id="Instrumental-variable-estimators-of-causal-effects">
<h1>Instrumental variable estimators of causal effects<a class="headerlink" href="#Instrumental-variable-estimators-of-causal-effects" title="Permalink to this headline">¶</a></h1>
<p><strong>Overview</strong></p>
<ul class="simple">
<li><p>Causal effect estimation with a binary IV</p></li>
<li><p>Traditional IV estimators</p></li>
<li><p>Instrumental variable estimators in the presence of individual-level heterogeneity</p></li>
<li><p>Conclusions</p></li>
</ul>
<div class="section" id="Causal-effect-estimation-with-a-binary-IV">
<h2>Causal effect estimation with a binary IV<a class="headerlink" href="#Causal-effect-estimation-with-a-binary-IV" title="Permalink to this headline">¶</a></h2>
<p>We consider the standard relationship</p>
<p><span class="math">\begin{align*}
Y = \alpha + \delta D +\epsilon,
\end{align*}</span></p>
<p>where <span class="math notranslate nohighlight">\(\delta\)</span> is the true causal effect that (for now) is assumed to be <strong>constant</strong>.</p>
<p><img alt="03fe28b4566c4b73afac3edbc93f0353" class="no-scaled-link" src="../../_images/fig-9-1.png" style="width: 500px;" /></p>
<ul class="simple">
<li><p>No conditioning estimator would effectively estimate the causal effect of <span class="math notranslate nohighlight">\(D\)</span> on <span class="math notranslate nohighlight">\(Y\)</span> because no observed variable satisfy the back-door criterion.</p></li>
<li><p>If perfect stratification cannot be be enacted with the available data, one possible solution is to find an exogenous source of variation that determines <span class="math notranslate nohighlight">\(Y\)</span> only by way of the causal variable <span class="math notranslate nohighlight">\(D\)</span>. The causal effect is then estimated by measuring how much <span class="math notranslate nohighlight">\(Y\)</span> varies with the proportion of the total variation in <span class="math notranslate nohighlight">\(D\)</span> that is attributable to the exogenous variation.</p></li>
</ul>
<p><span class="math">\begin{align*}
E[Y] = E[\alpha + \delta D + \epsilon] = \alpha + \delta E[D] + E[\epsilon]
\end{align*}</span></p>
<p>We can rewrite this as a difference equation in <span class="math notranslate nohighlight">\(Z\)</span>:</p>
<p><span class="math">\begin{align*}
E[Y \mid Z = 1] - E[Y \mid Z = 0] = \delta (E[D \mid Z = 1] - E[D \mid Z = 0]) + (E[\epsilon \mid Z = 1] - E[\epsilon \mid Z = 0])
\end{align*}</span></p>
<p>Then we divide both sides by <span class="math notranslate nohighlight">\(E[D \mid Z = 1] - E[D \mid Z = 0]\)</span>.</p>
<p><span class="math">\begin{align*}
\frac{E[Y \mid Z = 1] - E[Y \mid Z = 0]}{E[D \mid Z = 1] - E[D \mid Z = 0]} =  \frac{\delta (E[D \mid Z = 1] - E[D \mid Z = 0]) + (E[\epsilon \mid Z = 1] - E[\epsilon \mid Z = 0])}{E[D \mid Z = 1] - E[D \mid Z = 0]}
\end{align*}</span></p>
<p>If Figure 9.1 (a) is an accurate description of the causal structure, then <span class="math notranslate nohighlight">\(E[\epsilon \mid Z = 1] = E[\epsilon \mid Z = 0] = 0\)</span>.</p>
<p><span class="math">\begin{align*}
\frac{E[Y \mid Z = 1] - E[Y \mid Z = 0]}{E[D \mid Z = 1] - E[D \mid Z = 0]}  = \delta
\end{align*}</span></p>
<p><span class="math">\begin{align*}
\hat{\delta}_{IV, WALD} = \frac{E[Y \mid Z = 1] - E[Y \mid Z = 0]}{E[D \mid Z = 1] - E[D \mid Z = 0]}
\end{align*}</span></p>
<ul class="simple">
<li><p>The assumption that <span class="math notranslate nohighlight">\(\delta\)</span> is an invariant structural effect is crucial for this result.</p></li>
</ul>
</div>
<div class="section" id="Demonstration-dataset">
<h2>Demonstration dataset<a class="headerlink" href="#Demonstration-dataset" title="Permalink to this headline">¶</a></h2>
<p>We wish to determine whether private high school outperform public high schools as measured by <span class="math notranslate nohighlight">\(9^{th}\)</span> grade achievement tests. There exists a school voucher program in the city that covers tuition in case one attends private school. However, there are budgetary limits and so the vouchers are available only to 10% of students and allocated by a lottery.</p>
<p><img alt="f43306f33a994c01b364694f421a5d75" class="no-scaled-link" src="../../_images/fig-table-9-1.png" style="width: 500px;" /></p>
<ul class="simple">
<li><p>Winning the lottery increases private school attendance.</p></li>
</ul>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">get_sample_iv_demonstration</span><span class="p">():</span>
    <span class="sd">&quot;&quot;&quot;Simulates sample.</span>

<span class="sd">    Simulates a sample of 10,000 individuals for the IV demonstration</span>
<span class="sd">    based on the information provided in our textbook.</span>

<span class="sd">    Notes:</span>

<span class="sd">        The school administration distributed 1,000 vouchers for</span>
<span class="sd">        private school attendance in order to shift students</span>
<span class="sd">        from public into private school. The goals is to increase</span>
<span class="sd">        educational achievement.</span>

<span class="sd">    Args:</span>
<span class="sd">        None</span>

<span class="sd">    Returns:</span>
<span class="sd">        A pandas Dataframe with the observable characteristics (Y, D, Z)</span>
<span class="sd">        for all individuals.</span>

<span class="sd">        Y: standardized test for 9th graders</span>
<span class="sd">        D: private school attendance</span>
<span class="sd">        Z: voucher available</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We first initialize an empty Dataframe with 10,000 rowns and three</span>
    <span class="c1"># columns.</span>
    <span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">]</span>
    <span class="n">index</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">Index</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">),</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;Identifier&quot;</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="n">index</span><span class="p">)</span>

    <span class="c1"># We sample the exact number of individuals following the description</span>
    <span class="c1"># in Table 9.2.</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">10000</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">8000</span><span class="p">:</span>
            <span class="n">y</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">9000</span><span class="p">:</span>
            <span class="n">y</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">60</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span>
        <span class="k">elif</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="mi">9800</span><span class="p">:</span>
            <span class="n">y</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">50</span><span class="p">),</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">y</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="mi">58</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span>

        <span class="n">df</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="p">[</span><span class="n">y</span><span class="p">,</span> <span class="n">d</span><span class="p">,</span> <span class="n">z</span><span class="p">]</span>

    <span class="c1"># We shuffle all rows so we do not have the different subsamples</span>
    <span class="c1"># grouped together.</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">reset_index</span><span class="p">(</span><span class="n">drop</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="c1"># We set the types of our columns for prettier formatting later.</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>
    <span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">astype</span><span class="p">({</span><span class="s2">&quot;D&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">int</span><span class="p">})</span>

    <span class="k">return</span> <span class="n">df</span>
</pre></div>
</div>
</div>
<p>Let’s have a look at the structure of the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span> <span class="o">=</span> <span class="n">get_sample_iv_demonstration</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Y</th>
      <th>D</th>
      <th>Z</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>48.606920</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>50.240003</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>49.377337</td>
      <td>0</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>60.885880</td>
      <td>1</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>50.160785</td>
      <td>0</td>
      <td>0</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>How about the conditional distribution of observed outcomes?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;Z&quot;</span><span class="p">])[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
D  Z
0  0    50.009760
   1    49.962199
1  0    60.034692
   1    58.072959
Name: Y, dtype: float64
</pre></div></div>
</div>
<p>We can always run an OLS regression first to get a rough sense of the data.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">rslt</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.904</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.904</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>9.374e+04</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 09 Jun 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>
</tr>
<tr>
  <th>Time:</th>                 <td>09:34:37</td>     <th>  Log-Likelihood:    </th> <td> -14482.</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>2.897e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  9998</td>      <th>  BIC:               </th> <td>2.898e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>Intercept</th> <td>   50.0054</td> <td>    0.011</td> <td> 4555.317</td> <td> 0.000</td> <td>   49.984</td> <td>   50.027</td>
</tr>
<tr>
  <th>D</th>         <td>    9.7023</td> <td>    0.032</td> <td>  306.173</td> <td> 0.000</td> <td>    9.640</td> <td>    9.764</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>12.957</td> <th>  Durbin-Watson:     </th> <td>   2.022</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.002</td> <th>  Jarque-Bera (JB):  </th> <td>  13.196</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.074</td> <th>  Prob(JB):          </th> <td> 0.00136</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 3.100</td> <th>  Cond. No.          </th> <td>    3.13</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div>
</div>
<p>However, to exploiting the structure of the dataset, we rather want to compute the IV estimate.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">def</span> <span class="nf">get_wald_estimate</span><span class="p">(</span><span class="n">df</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate Wald estimate.</span>

<span class="sd">    Calculates the Wald estimate for the causal effect of treatment</span>
<span class="sd">    on an observed outcome using a binary instrument.</span>

<span class="sd">    Args:</span>
<span class="sd">        df: A pandas DataFrame</span>

<span class="sd">    Returns:</span>
<span class="sd">        A float with the estimated causal effect.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># We compute the average difference in observed outcomes.</span>
    <span class="n">average_outcome</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Z&quot;</span><span class="p">)[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="n">numerator</span> <span class="o">=</span> <span class="n">average_outcome</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">average_outcome</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="c1"># We compute the average difference in treatment uptake.</span>
    <span class="n">average_treatment</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">(</span><span class="s2">&quot;Z&quot;</span><span class="p">)[</span><span class="s2">&quot;D&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span><span class="o">.</span><span class="n">to_dict</span><span class="p">()</span>
    <span class="n">denominator</span> <span class="o">=</span> <span class="n">average_treatment</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="o">-</span> <span class="n">average_treatment</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">rslt</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>

    <span class="k">return</span> <span class="n">rslt</span>
</pre></div>
</div>
</div>
<p>So, let’s see.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">rslt</span> <span class="o">=</span> <span class="n">get_wald_estimate</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Wald estimate: </span><span class="si">{:5.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">rslt</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
 Wald estimate: 5.183
</pre></div></div>
</div>
</div>
<div class="section" id="Traditional-IV-estimators">
<h2>Traditional IV estimators<a class="headerlink" href="#Traditional-IV-estimators" title="Permalink to this headline">¶</a></h2>
<p>We now move beyond a binary instrument.</p>
<p><span class="math">\begin{align*}
\hat{\delta}_{IV} \equiv \frac{Cov_N(y_i, z_i)}{Cov_N(d_i, z_i)}
\end{align*}</span></p>
<p>Moving towards the population-level relationships:</p>
<p><span class="math">\begin{align*}
\frac{Cov(Y, Z)}{Cov(D, Z)} & = \frac{\delta Cov(D, Z) + Cov[\epsilon, Z]}{Cov(D, Z)} \\
& = \delta
\end{align*}</span></p>
<p>So, this suggests that:</p>
<p><span class="math">\begin{align*}
\frac{Cov(Y, Z)}{Cov(D, Z)}  \xrightarrow{p} \delta
\end{align*}</span></p>
<p><img alt="ec11c7711f2843bfb7821f95e38630a0" class="no-scaled-link" src="../../_images/fig-9-2.png" style="width: 500px;" /></p>
<p>Returning to our simulated example, we can now apply the two-stage least squares (2SLS) estimator you are familiar with.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;D_pred&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;D ~ Z&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D_pred&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[36]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.002</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.002</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   17.39</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 09 Jun 2020</td> <th>  Prob (F-statistic):</th> <td>3.07e-05</td>
</tr>
<tr>
  <th>Time:</th>                 <td>09:59:20</td>     <th>  Log-Likelihood:    </th> <td> -26171.</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>5.235e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  9998</td>      <th>  BIC:               </th> <td>5.236e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>Intercept</th> <td>   50.5478</td> <td>    0.153</td> <td>  330.860</td> <td> 0.000</td> <td>   50.248</td> <td>   50.847</td>
</tr>
<tr>
  <th>D_pred</th>    <td>    5.1830</td> <td>    1.243</td> <td>    4.170</td> <td> 0.000</td> <td>    2.747</td> <td>    7.619</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td>3794.296</td> <th>  Durbin-Watson:     </th> <td>   1.993</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th>  <td> 0.000</td>  <th>  Jarque-Bera (JB):  </th> <td>11127.197</td>
</tr>
<tr>
  <th>Skew:</th>           <td> 2.065</td>  <th>  Prob(JB):          </th> <td>    0.00</td>
</tr>
<tr>
  <th>Kurtosis:</th>       <td> 6.107</td>  <th>  Cond. No.          </th> <td>    38.0</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div>
</div>
<p>Given the structure of our example, both estimators are equivalent. As of now, <a class="reference external" href="https://www.statsmodels.org">statsmodels</a> does not provide good support for the instrumental variables estimation. That is true for a host of methods often used by economists. Often <a class="reference external" href="https://bashtage.github.io/linearmodels/doc/index.html">linearmodels</a> provides a viable alternative.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">linearmodels</span> <span class="kn">import</span> <span class="n">IV2SLS</span>  <span class="c1"># noqa: E402</span>

<span class="n">df</span><span class="p">[</span><span class="s2">&quot;const&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">IV2SLS</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;const&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;D&quot;</span><span class="p">],</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Z&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[37]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>IV-2SLS Estimation Summary</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>0.7076</td>
</tr>
<tr>
  <th>Estimator:</th>             <td>IV-2SLS</td>     <th>  Adj. R-squared:    </th> <td>0.7075</td>
</tr>
<tr>
  <th>No. Observations:</th>       <td>10000</td>      <th>  F-statistic:       </th> <td>77.109</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, Jun 09 2020</td> <th>  P-value (F-stat)   </th> <td>0.0000</td>
</tr>
<tr>
  <th>Time:</th>                 <td>09:59:21</td>     <th>  Distribution:      </th> <td>chi2(1)</td>
</tr>
<tr>
  <th>Cov. Estimator:</th>        <td>robust</td>      <th>                     </th>    <td></td>
</tr>
<tr>
  <th></th>                          <td></td>         <th>                     </th>    <td></td>
</tr>
</table>
<table class="simpletable">
<caption>Parameter Estimates</caption>
<tr>
    <td></td>    <th>Parameter</th> <th>Std. Err.</th> <th>T-stat</th> <th>P-value</th> <th>Lower CI</th> <th>Upper CI</th>
</tr>
<tr>
  <th>const</th>  <td>50.548</td>    <td>0.0748</td>   <td>676.20</td> <td>0.0000</td>   <td>50.401</td>   <td>50.694</td>
</tr>
<tr>
  <th>D</th>      <td>5.1830</td>    <td>0.5902</td>   <td>8.7812</td> <td>0.0000</td>   <td>4.0261</td>   <td>6.3398</td>
</tr>
</table><br/><br/>Endogenous: D<br/>Instruments: Z<br/>Robust Covariance (Heteroskedastic)<br/>Debiased: False<br/>id: 0x7f9d353b4490</div>
</div>
</div>
<div class="section" id="Instrumental-variable-estimators-in-the-presence-of-individual-level-heterogeneity">
<h2>Instrumental variable estimators in the presence of individual-level heterogeneity<a class="headerlink" href="#Instrumental-variable-estimators-in-the-presence-of-individual-level-heterogeneity" title="Permalink to this headline">¶</a></h2>
<p><span class="math">\begin{align*}
Y & = Y^0 + D (Y^1 - Y^0)  \\
& = Y^0 + \delta D \\
& = \mu^0 + \delta D + \nu^0,
\end{align*}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mu^0 \equiv E[Y^0]\)</span> and <span class="math notranslate nohighlight">\(\nu^0 \equiv Y^0 - E[Y^0]\)</span>. Here, <span class="math notranslate nohighlight">\(\delta\)</span> now has a clear interpretation.</p>
<p>We need to add a four-category latent variable <span class="math notranslate nohighlight">\(C\)</span>:</p>
<p><span class="math">\begin{align*}
\text{Compliers (C = c)} & : D^{Z = 0} = 0 \,\text{and}\, D^{Z = 1} = 1  \\
\text{Defiers (C = d)} & : D^{Z = 0} = 1 \,\text{and}\, D^{Z = 1} = 0  \\
\text{Always takers (C = a)} & : D^{Z = 0} = 1 \,\text{and}\, D^{Z = 1} = 1  \\
\text{Never takers (C = n)} & : D^{Z = 0} = 0 \,\text{and}\, D^{Z = 1} = 0  \\
\end{align*}</span></p>
<p>Analogously to the definition of the observed outcome, <span class="math notranslate nohighlight">\(Y\)</span>, the observed treatment indicator variable <span class="math notranslate nohighlight">\(D\)</span> can then be defined as</p>
<p><span class="math">\begin{align*}
D & =  D^{Z = 0} + (D^{Z = 1} - D^{Z = 0}) Z \\
&=  D^{Z = 0} + \kappa Z
\end{align*}</span></p>
<p>What is the value of <span class="math notranslate nohighlight">\(\kappa\)</span> for the different latent groups?</p>
<p><strong>Identifying assumptions for the Local Average Treatment Effect</strong></p>
<ul class="simple">
<li><p>Independence, <span class="math notranslate nohighlight">\((Y^1, Y^0, D^{Z = 1}, D^{Z = 0}) \perp \!\!\! \perp Z\)</span></p></li>
<li><p>Nonzero effect of instrument, <span class="math notranslate nohighlight">\(\kappa \neq 0\)</span> for at least some <span class="math notranslate nohighlight">\(i\)</span></p></li>
<li><p>Monotonicty assumption, either <span class="math notranslate nohighlight">\(\kappa \geq 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span> or <span class="math notranslate nohighlight">\(\kappa \leq 0\)</span> for all <span class="math notranslate nohighlight">\(i\)</span></p></li>
</ul>
<p>If these assumptions are valid, then an instrument <span class="math notranslate nohighlight">\(Z\)</span> identifies the <span class="math notranslate nohighlight">\(LATE\)</span>: the average treatment effect for the subset of the population whose treatment selection is induced by the treatment.</p>
<p><span class="math">\begin{align*}
\hat{\delta}_{IV,WALD} \xrightarrow{p} E [\delta \mid C = c]
\end{align*}</span></p>
<p><img alt="67b1e5f5782a486292949ead01e4b735" class="no-scaled-link" src="../../_images/fig-table-9-2.png" style="width: 500px;" /></p>
<p>What can we learn about the different latent groups?</p>
<ul class="simple">
<li><p>Monotonicity, there are no defiers</p></li>
<li><p>Independence, the same distribution of never takes, always takers, and compliers is present among voucher groups</p></li>
</ul>
<p><span class="math">\begin{align*}
\frac{Pr_N [d_i = 1, z_i = 0]}{Pr_N[z_i = 0]} \xrightarrow{p} Pr[C = a] \\
\frac{Pr_N [d_i = 0, z_i = 1]}{Pr_N[z_i = 1]} \xrightarrow{p} Pr[C = n] \\
\end{align*}</span></p>
<p>We also know <span class="math notranslate nohighlight">\(Pr[C = d] = 0\)</span> and thus</p>
<p><span class="math">\begin{align*}
1 - \frac{Pr_N [d_i = 1, z_i = 0]}{Pr_N[z_i = 0]} - \frac{Pr_N [d_i = 0, z_i = 1]}{Pr_N[z_i = 1]}\xrightarrow{p} Pr[C = c] \\
\end{align*}</span></p>
<p><img alt="55a05e44bc6546149aadcfb7279e9215" class="no-scaled-link" src="../../_images/fig-table-9-3.png" style="width: 500px;" /></p>
<p>How can we learn about the LATE from the information analyzed so far?</p>
<p><span class="math">\begin{align*}
E[\delta \mid C = c] = E [Y^1 - Y^0 \mid C = c]
\end{align*}</span></p>
<p>Let’s start with the following:</p>
<p><span class="math">\begin{align*}
E[Y \mid D = 1, Z = 1] & = \frac{Pr[C = c]}{Pr[C = c] + Pr [C=a]} E[Y^1 \mid C = c] \\
&+ \frac{Pr[C = a]}{Pr[C = c] + Pr [C=a]} E[Y^1 \mid C = a] \\
&\\
E[Y \mid D = 0, Z = 0] & = \frac{Pr[C = c]}{Pr[C = c] + Pr [C=n]} E[Y^0 \mid C = c] \\
& + \frac{Pr[C = n]}{Pr[C = c] + Pr [C=n]} E[Y^0 \mid C = n]
\end{align*}</span></p>
<p>Note that we can consistent estimates for <span class="math notranslate nohighlight">\(E[Y^0 \mid C = n]\)</span> and <span class="math notranslate nohighlight">\(E[Y^1 \mid C = a]\)</span> are provided in the table directly.</p>
<p>Now lets tie this back to the Wald estimator:</p>
<p><span class="math">\begin{align*}
\hat{\delta}_{IV, WALD} = \frac{E[Y \mid Z = 1] - E[Y \mid Z = 0]}{E[D \mid Z = 1] - E[D \mid Z = 0]}
\end{align*}</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">get_shares_latent_groups</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_instrumental-variable_notebook_26_0.png" src="../../_images/lectures_instrumental-variable_notebook_26_0.png" />
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">get_outcome_latent_groups</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_instrumental-variable_notebook_27_0.png" src="../../_images/lectures_instrumental-variable_notebook_27_0.png" />
</div>
</div>
</div>
<div class="section" id="Criticism">
<h2>Criticism<a class="headerlink" href="#Criticism" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p>instrument-dependent parameter</p></li>
<li><p>limited policy-relevance</p></li>
</ul>
</div>
<div class="section" id="Discussion">
<h2>Discussion<a class="headerlink" href="#Discussion" title="Permalink to this headline">¶</a></h2>
<p>We revisit and discuss the discussion of the LATE’s usefulness.</p>
<p><img alt="5220acf414bc44b79ef17d0c9f730e9a" class="no-scaled-link" src="../../_images/fig-angrist-1990-cover.png" style="width: 700px;" /></p>
<p><img alt="648508e875824bfd8d92dde8be0b46fd" class="no-scaled-link" src="../../_images/fig-angrist-1990-valid.png" style="width: 500px;" /></p>
<p><strong>Table of contents</strong></p>
<ul class="simple">
<li><p>Introduction</p></li>
<li><p>Background and Data</p>
<ul>
<li><p>National Random Selection</p></li>
<li><p>Social Security Earnings Data</p></li>
</ul>
</li>
<li><p>The Effect of Draft Eligibility and Earnings</p></li>
<li><p>The Effect of Military Service on Earnings</p>
<ul>
<li><p>Estimates Using Draft Eligibility</p></li>
<li><p>Efficient Instrumental Variables Estimates</p></li>
</ul>
</li>
<li><p>Military Service of Labor Market Experience</p></li>
<li><p>Caveats</p>
<ul>
<li><p>Treatment Effect Heterogeneity</p></li>
<li><p>The Absence of Covariates</p></li>
<li><p>Earnings-Modifying Draft Avoidance Behavior</p></li>
</ul>
</li>
<li><p>Conclusions</p></li>
</ul>
<p><img alt="71df8c1bae81404089cfb5bb1d2e9139" class="no-scaled-link" src="../../_images/fig-angrist-krueger-1991-cover.png" style="width: 700px;" /></p>
<p><img alt="ea674b2f958240208c2e75d4a1d9cc0b" class="no-scaled-link" src="../../_images/fig-angrist-krueger-1991-valid.png" style="width: 500px;" /></p>
<p><strong>Table of contents</strong></p>
<ul class="simple">
<li><p>Introduction</p></li>
<li><p>Season of Birth, Compulsory Schooling, and Years of Education</p>
<ul>
<li><p>Direct Evidence and teh Effect of Compulsory Schooling Laws</p></li>
<li><p>Why do Compulsory Schooling Laws Work?</p></li>
</ul>
</li>
<li><p>Estimating the Returns to Education</p>
<ul>
<li><p>TSLS Estimation</p></li>
<li><p>Allowing the Seasonal Pattern in Education to Vary by State of Birth</p></li>
<li><p>Estimates for Black Men</p></li>
</ul>
</li>
<li><p>Other Possible Effects of Season of Birth</p></li>
<li><p>Conclusions</p></li>
</ul>
<p><img alt="0e6f4823665e47f88cd53003eb5549c0" class="no-scaled-link" src="../../_images/fig-rosenzweig-wolpin-2000-cover.png" style="width: 700px;" /></p>
<p>We discuss Rosenzweig &amp; Wolpin (2000) in more detail because it provides a small structural economic model of schooling choice that allows to interpret the instrumental variable estimates of Angrist (1990) and Angrist &amp; Krueger (1991).</p>
<p><span class="math">\begin{align*}\begin{array}{ll}
a &\text{age} \\
y_a & \text{earnings at age a} \\
S & \text{level of schooling attainment} \\
X_a & \text{work experience at age $a$} \\
\mu & \text{ability} \\
a_e & \text{school entry age} \\
a_\kappa & \text{minimum age to leave school}\\
S_0 = a_\kappa - a_e &  \text{minimum schooling} \\
c  & \text{direct cost of education}
\end{array}\end{align*}</span></p>
<p>Wages are determined as follows: <span class="math">\begin{align*}
\ln y_a = f(S, \mu) + g(X_a, \mu)
\end{align*}</span></p>
<p>The authors assume that individuals work full-time after school and there is no uncertainty about wages. Individuals decide whether to pursue one additional year of schooling after the mandatory minimum. If they do so <span class="math notranslate nohighlight">\(s_1\)</span> takes value one and zero otherwise. So, the final level of schooling is <span class="math notranslate nohighlight">\(S_1 = S_0 + s_1\)</span>. All individuals work <span class="math notranslate nohighlight">\(A\)</span> periods in the labor market. Spending one additional year in school does not reduce total time in the labor market. However, it results in
entering the labor market one year later as schooling precludes working. Ability is the only source of heterogeneity and distributed at random in the population.</p>
<p>The individual’s objective is to choose their final level of schooling such as to maximize their discounted lifetime earnings under the two scenarios <span class="math notranslate nohighlight">\((V_1, V_0)\)</span>.</p>
<p><span class="math">\begin{align*}\begin{array}{ll}
V_1(S_1 = 1 | S_0) &= -c + \sum_{a=0}^{A-1} \beta^{a + 1} y_a \\
&= -c + \sum_{a=0}^{A-1} \beta^{a + 1} \exp(f(S_0 + 1, \mu) + g(a, \mu)) \\
&= -c + \sum_{a=0}^{A-1} \beta^{a + 1} \exp(f(S_0 + 1, \mu)) \exp(g(a, \mu)) \\
&= -c + \exp(f(S_0 + 1, \mu)) \sum_{a=0}^{A-1} \beta^{a + 1} \exp(g(a, \mu))
\end{array}\end{align*}</span></p>
<p><span class="math">\begin{align*}
V_1(S_1 = 0 | S_0) &= \sum_{a=0}^{A-1} \beta^a y_a \\
&= \exp(f(S_0, \mu)) \sum_{a=0}^{A-1} \beta^a \exp(g(a, \mu))
\end{align*}</span></p>
<p>We now turn attention to the decision rule <span class="math notranslate nohighlight">\(V_1 &gt; V_0\)</span> implies further pursuit of education.</p>
<p><span class="math">\begin{align*}\begin{array}{ll}
-c + \exp(f(S_0 + 1, \mu) \sum_{a=0}^{A-1} \beta^{a + 1} \exp(g(a, \mu)) \\
> \exp(f(S_0 + 1, \mu)) \sum_{a=0}^{A-1} \beta^a \exp(g(a, \mu) \\[20pt]
-c + \exp(f(S_0 + 1, \mu) \sum_{a=0}^{A-1} \beta^{a + 1} \exp(g(a, \mu)) \\
> \underbrace{\exp(f(S_0 + 1, \mu)) \sum_{a=0}^{A-1} \beta^a \exp(g(a, \mu)}_{V_1(S_1 = 0 | S_0)}
\end{array}\end{align*}</span></p>
<p>now divide by <span class="math notranslate nohighlight">\(V_1(S_1 = 0 | S_0)\)</span></p>
<p><span class="math">\begin{align*}\begin{array}{ll}
\frac{\exp(f(S_0 + 1, \mu))}{\exp(f(S_0, \mu))} \beta &> 1 + \frac{c}{V_1(S_1 = 0 | S_0)} \\
&> (1 + \frac{c}{V_1(S_1 = 0 | S_0)}) (1 + r) \\
f(S_0 + 1, \mu) - f(S_0, \mu) &> r + \frac{c}{V_1(S_1 = 0 | S_0)}
\end{array}\end{align*}</span></p>
<p>using <span class="math notranslate nohighlight">\(\ln (1 + x) \approx x\)</span> for small <span class="math notranslate nohighlight">\(x\)</span>.</p>
<p><span class="math">\begin{align*}
s_1 = \begin{cases}
1 & \text{if}\quad f(S_0 + 1, \mu) - f(S_0, \mu) \geq r + \ln\left(\frac{c}{V_1(s_1 = 0 \mid S_0)} + 1\right) \\
0 & \text{otherwise}
\end{cases}
\end{align*}</span></p>
<p>If ability increases the marginal schooling return, then there exists a unique cutoff value for ability <span class="math notranslate nohighlight">\(\mu^*\)</span> such that individuals with ability above the cutoff continue schooling while those below do not.</p>
<p><span class="math">\begin{align*}
\frac{\partial f(S_0 + 1, \mu) - f(S_0, \mu)}{\partial \mu} > 0
\end{align*}</span></p>
<p>Even if randomly assigned, optimizing behavior induces an association between schooling and ability. This generates the ability bias. % <span class="math">\begin{align*}
E[ f(S_0 + 1, \mu)  \mid \mu > \mu^*] - E[ f(S_0, \mu)  \mid \mu < \mu^*] > E[ f(S_0 + 1, \mu) ] - E[ f(S_0, \mu)]
\end{align*}</span></p>
<p>We now turn to the development of the Wald estimator Wald (1940). So, we first derive expected earnings equation for each age <span class="math notranslate nohighlight">\(a\)</span>.</p>
<p><span class="math">\begin{align*}
E[\ln y_a] = \pi_1 [f(S_0 + 1, \mu_1) + g(a - a_\kappa - 1, \mu_1)]
+ (1 - \pi_1) [f(S_0, \mu_2) + g(a - a_\kappa, \mu_2)]
\end{align*}</span></p>
<p>We now consider the following scenario, where we reduce the school entry age by one year but keep the minimum school leaving age unchanged. Type 1 achieve their optimal level of schooling exactly at the school leaving age. Type 2’s will be forced to attend school a year longer. % <span class="math">\begin{align*}
E[\ln y_a] = \pi_1 [f(S_0 + 1, \mu_1) + g(a - a_\kappa, \mu_1)]
+ (1 - \pi_1) [f(S_0 + 1, \mu_2) + g(a - a_\kappa, \mu_2)]
\end{align*}</span></p>
<p>The difference in expected (ln) earnings divided by the difference in expected schooling <span class="math notranslate nohighlight">\(0 \cdot \pi_1 + 1 \cdot (1 - \pi_1)\)</span>, the Wald estimator, is thus</p>
<p><span class="math">\begin{align*}\begin{array}{ll}
E[\ln y_a | \underbrace{Z = 1}_{\text{reduced entry age}}] - E[\ln y_a | Z = 0] \\
= \pi_1 (f(S_0 + 1, \mu_1) + g(a - a_\kappa, \mu_1)) \\
\hspace{11pt}+ (1 - \pi     _1) (f(S_0 + 1, \mu_2) + g(a - o_\kappa, \mu_2)) \\
\hspace{11pt}- \pi_1 (f(S_0 + 1, \mu_1) + g(a - a_\kappa - 1, \mu_1)) \\
\hspace{11pt}- (1 - \pi_1) (f(S_0, \mu_2) + g(a - o_\kappa, \mu_2)) \\[20pt]
= \pi_1 (g(a - a_\kappa, \mu_1) - g(a - a_\kappa - 1, \mu_1)) \\
\hspace{11pt}+ (1 - \pi_1) (f(S_0 + 1, \mu_2) - f(S_0, \mu_2))
\end{array}\end{align*}</span></p>
<p>divide by difference in schooling attainment</p>
<p><span class="math">\begin{align*}
\pi_1 * 0 + (1 - \pi_1) * 1
\end{align*}</span></p>
<p><span class="math">\begin{align*}
\frac{\Delta E (\ln y_a)}{\Delta S} = \underbrace{\frac{\pi_1}{1 -\pi_1} [g(a - a_\kappa, \mu_1) - g(a - a_\kappa - 1, \mu_1)]}_{\text{type 1's additional experience}} + \underbrace{[f(S_0 + 1, \mu_2) - f(S_0, \mu_2)]}_{\text{effect of interest (compliers only)}},
\end{align*}</span></p>
<p>where <span class="math notranslate nohighlight">\(\frac{\Delta E (\ln y_a)}{\Delta S}\)</span> corresponds to <span class="math notranslate nohighlight">\(E(\ln y_a \mid Z = 1) - E(\ln y_a \mid Z = 0)\)</span> and <span class="math notranslate nohighlight">\(Z\)</span> takes value one under the reduced school entry age and zero otherwise. Thus the estimate does not correspond directly to the effect of interest. However, Angrist &amp; Krueger (1991) make the point in Figure V that for the cohort they are looking at <span class="math notranslate nohighlight">\((a = 40, ..., 49)\)</span> the effect of age on earnings is negligible.</p>
</div>
<div class="section" id="Resources">
<h2>Resources<a class="headerlink" href="#Resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Angrist, J. D. (1990)</strong>. <a class="reference external" href="https://www.jstor.org/stable/2006669?seq=1">Lifetime earnings and the vietnam era draft lottery: Evidence from social security records</a>. <em>American Economic Review</em>, 80(3), 313–336.</p></li>
<li><p><strong>Angrist, J. D., &amp; Imbens, G. W. (1999)</strong>. <a class="reference external" href="https://www.jstor.org/stable/146178?seq=1">Instrumental variables: A study of implicit behavioral assumptions used in making program evaluations</a>. <em>Journal of Human Resources</em>, 34(4), 823– 827.</p></li>
<li><p><strong>Angrist, J. D., &amp; Krueger, A. B. (1991)</strong>. <a class="reference external" href="https://www.researchgate.net/publication/24091331_Does_Compulsory_School_Attendance_Affect_Schooling_and_Earnings">Does compulsory school attendance affect schooling and earnings?</a>. <em>The Quarterly Journal of Economics</em>, 106(4), 979-1014.</p></li>
<li><p><strong>Heckman, J. J. (1997)</strong>. <a class="reference external" href="https://www.jstor.org/stable/146178?seq=1">Instrumental variables: A study of implicit behavioral assumptions used in making program evaluations</a>. <em>The Journal of Human Resources</em>, 32(3), 441–462.</p></li>
<li><p><strong>Rosenzweig, M. R., &amp; Wolpin, K. I. (2000)</strong>. <a class="reference external" href="https://www.aeaweb.org/articles?id=10.1257/jel.38.4.827">Natural ”natural” experiments in economics</a>. <em>Journal of Economic Literature</em>, 38(4), 827–874.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../generalized-roy-model/notebook.html" class="btn btn-neutral float-right" title="The Generalized Roy Model" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../selection-heterogeneity-graphs/notebook.html" class="btn btn-neutral float-left" title="Self-selection, heterogeneity, and causal graphs" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Prof. Dr. Philipp Eisenhauer.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>