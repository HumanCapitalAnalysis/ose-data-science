

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8" />
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  
  <title>Regression estimators of causal effects &mdash; OSE data science  documentation</title>
  

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />

  
  

  
  

  

  
  <!--[if lt IE 9]>
    <script src="../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
        <script src="../../_static/jquery.js"></script>
        <script src="../../_static/underscore.js"></script>
        <script src="../../_static/doctools.js"></script>
        <script src="../../_static/language_data.js"></script>
        <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
        <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
        <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["$", "$"], ["\\(", "\\)"]], "processEscapes": true, "ignoreClass": "document", "processClass": "math|output_area"}})</script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Self-selection, heterogeneity, and causal graphs" href="../selection-heterogeneity-graphs/notebook.html" />
    <link rel="prev" title="Matching estimators of causal effects" href="../matching-estimators/notebook.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> OSE data science
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul class="current">
<li class="toctree-l1 current"><a class="reference internal" href="../index.html">Lectures</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="../index.html#introduction">Introduction</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#potential-outcome-model">Potential outcome model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#causal-graphs">Causal graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#conditioning-estimators">Conditioning estimators</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#matching-estimators">Matching estimators</a></li>
<li class="toctree-l2 current"><a class="reference internal" href="../index.html#regression-estimators">Regression estimators</a><ul class="current">
<li class="toctree-l3 current"><a class="current reference internal" href="#">Regression estimators of causal effects</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#Regression-as-a-descriptive-tool">Regression as a descriptive tool</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Regression-adjustment-as-a-strategy-to-estimate-causal-effects">Regression adjustment as a strategy to estimate causal effects</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Potential-outcomes-and-omitted-variable-bias">Potential outcomes and omitted-variable bias</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Regression-as-adjustment-for-otherwise-omitted-variables">Regression as adjustment for otherwise omitted variables</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Freedman’s-paradox">Freedman’s paradox</a></li>
<li class="toctree-l4"><a class="reference internal" href="#Resources">Resources</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#heterogeneity-selection-and-causal-graphs">Heterogeneity, selection, and causal graphs</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#instrumental-variables">Instrumental variables</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#generalized-roy-model">Generalized Roy model</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#causal-explanations">Causal explanations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#repeated-observations">Repeated observations</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#regression-discontinuity-design">Regression discontinuity design</a></li>
<li class="toctree-l2"><a class="reference internal" href="../index.html#generalized-method-of-moments">Generalized method of moments</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../problem-sets/index.html">Problem sets</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../handouts/index.html">Handouts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../projects/index.html">Projects</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../datasets/index.html">Data sources</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../partners/index.html">Partners</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../organization/index.html">Organization</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">OSE data science</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          

















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html" class="icon icon-home"></a> &raquo;</li>
        
          <li><a href="../index.html">Lectures</a> &raquo;</li>
        
      <li>Regression estimators of causal effects</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
          
            <a href="../../_sources/lectures/regression-estimators/notebook.ipynb.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  
<style>
/* CSS for nbsphinx extension */

/* remove conflicting styling from Sphinx themes */
div.nbinput.container div.prompt *,
div.nboutput.container div.prompt *,
div.nbinput.container div.input_area pre,
div.nboutput.container div.output_area pre,
div.nbinput.container div.input_area .highlight,
div.nboutput.container div.output_area .highlight {
    border: none;
    padding: 0;
    margin: 0;
    box-shadow: none;
}

div.nbinput.container > div[class*=highlight],
div.nboutput.container > div[class*=highlight] {
    margin: 0;
}

div.nbinput.container div.prompt *,
div.nboutput.container div.prompt * {
    background: none;
}

div.nboutput.container div.output_area .highlight,
div.nboutput.container div.output_area pre {
    background: unset;
}

div.nboutput.container div.output_area div.highlight {
    color: unset;  /* override Pygments text color */
}

/* avoid gaps between output lines */
div.nboutput.container div[class*=highlight] pre {
    line-height: normal;
}

/* input/output containers */
div.nbinput.container,
div.nboutput.container {
    display: -webkit-flex;
    display: flex;
    align-items: flex-start;
    margin: 0;
    width: 100%;
}
@media (max-width: 540px) {
    div.nbinput.container,
    div.nboutput.container {
        flex-direction: column;
    }
}

/* input container */
div.nbinput.container {
    padding-top: 5px;
}

/* last container */
div.nblast.container {
    padding-bottom: 5px;
}

/* input prompt */
div.nbinput.container div.prompt pre {
    color: #307FC1;
}

/* output prompt */
div.nboutput.container div.prompt pre {
    color: #BF5B3D;
}

/* all prompts */
div.nbinput.container div.prompt,
div.nboutput.container div.prompt {
    width: 4.5ex;
    padding-top: 5px;
    position: relative;
    user-select: none;
}

div.nbinput.container div.prompt > div,
div.nboutput.container div.prompt > div {
    position: absolute;
    right: 0;
    margin-right: 0.3ex;
}

@media (max-width: 540px) {
    div.nbinput.container div.prompt,
    div.nboutput.container div.prompt {
        width: unset;
        text-align: left;
        padding: 0.4em;
    }
    div.nboutput.container div.prompt.empty {
        padding: 0;
    }

    div.nbinput.container div.prompt > div,
    div.nboutput.container div.prompt > div {
        position: unset;
    }
}

/* disable scrollbars on prompts */
div.nbinput.container div.prompt pre,
div.nboutput.container div.prompt pre {
    overflow: hidden;
}

/* input/output area */
div.nbinput.container div.input_area,
div.nboutput.container div.output_area {
    -webkit-flex: 1;
    flex: 1;
    overflow: auto;
}
@media (max-width: 540px) {
    div.nbinput.container div.input_area,
    div.nboutput.container div.output_area {
        width: 100%;
    }
}

/* input area */
div.nbinput.container div.input_area {
    border: 1px solid #e0e0e0;
    border-radius: 2px;
    /*background: #f5f5f5;*/
}

/* override MathJax center alignment in output cells */
div.nboutput.container div[class*=MathJax] {
    text-align: left !important;
}

/* override sphinx.ext.imgmath center alignment in output cells */
div.nboutput.container div.math p {
    text-align: left;
}

/* standard error */
div.nboutput.container div.output_area.stderr {
    background: #fdd;
}

/* ANSI colors */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-default-inverse-fg { color: #FFFFFF; }
.ansi-default-inverse-bg { background-color: #000000; }

.ansi-bold { font-weight: bold; }
.ansi-underline { text-decoration: underline; }


div.nbinput.container div.input_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight] > pre,
div.nboutput.container div.output_area div[class*=highlight].math,
div.nboutput.container div.output_area.rendered_html,
div.nboutput.container div.output_area > div.output_javascript,
div.nboutput.container div.output_area:not(.rendered_html) > img{
    padding: 5px;
    margin: 0;
}

/* fix copybtn overflow problem in chromium (needed for 'sphinx_copybutton') */
div.nbinput.container div.input_area > div[class^='highlight'],
div.nboutput.container div.output_area > div[class^='highlight']{
    overflow-y: hidden;
}

/* hide copybtn icon on prompts (needed for 'sphinx_copybutton') */
.prompt a.copybtn {
    display: none;
}

/* Some additional styling taken form the Jupyter notebook CSS */
div.rendered_html table {
  border: none;
  border-collapse: collapse;
  border-spacing: 0;
  color: black;
  font-size: 12px;
  table-layout: fixed;
}
div.rendered_html thead {
  border-bottom: 1px solid black;
  vertical-align: bottom;
}
div.rendered_html tr,
div.rendered_html th,
div.rendered_html td {
  text-align: right;
  vertical-align: middle;
  padding: 0.5em 0.5em;
  line-height: normal;
  white-space: normal;
  max-width: none;
  border: none;
}
div.rendered_html th {
  font-weight: bold;
}
div.rendered_html tbody tr:nth-child(odd) {
  background: #f5f5f5;
}
div.rendered_html tbody tr:hover {
  background: rgba(66, 165, 245, 0.2);
}

/* CSS overrides for sphinx_rtd_theme */

/* 24px margin */
.nbinput.nblast.container,
.nboutput.nblast.container {
    margin-bottom: 19px;  /* padding has already 5px */
}

/* ... except between code cells! */
.nblast.container + .nbinput.container {
    margin-top: -19px;
}

.admonition > p:before {
    margin-right: 4px;  /* make room for the exclamation icon */
}

/* Fix math alignment, see https://github.com/rtfd/sphinx_rtd_theme/pull/686 */
.math {
    text-align: unset;
}
</style>
<div class="admonition note">
<p>Download the notebook <a class="reference download external" download="" href="https://nbviewer.jupyter.org/github/HumanCapitalAnalysis/ose-data-science/blob/master/lecturesregression-estimatorsnotebook.ipynb"><code class="xref download docutils literal notranslate"><span class="pre">here</span></code></a>!
Interactive online version: <a class="reference external" href="https://mybinder.org/v2/gh/HumanCapitalAnalysis/ose-data-science/master?filepath=lecturesregression-estimatorsnotebook.ipynb"><img alt="binder" src="https://mybinder.org/badge_logo.svg" /></a></p>
</div>
<div class="section" id="Regression-estimators-of-causal-effects">
<h1>Regression estimators of causal effects<a class="headerlink" href="#Regression-estimators-of-causal-effects" title="Permalink to this headline">¶</a></h1>
<p><strong>Overview</strong></p>
<ul class="simple">
<li><p>Regression as a descriptive tool</p></li>
<li><p>Regression adjustment as a strategy to to estimate causal effects</p></li>
<li><p>Regression as conditional-variance-weighted matching</p></li>
<li><p>Regression as an implementation of a perfect stratification</p></li>
<li><p>Regression as supplemental adjustment when matching</p></li>
<li><p>Extensions and other perspectives</p></li>
<li><p>Conclusion</p></li>
</ul>
<p>We start with different ways of using regression</p>
<ul class="simple">
<li><p>descriptive tools</p>
<ul>
<li><p>Anscombe quartet</p></li>
</ul>
</li>
<li><p>estimating causal effects</p></li>
<li><p>Freedman’s paradox</p></li>
</ul>
<div class="section" id="Regression-as-a-descriptive-tool">
<h2>Regression as a descriptive tool<a class="headerlink" href="#Regression-as-a-descriptive-tool" title="Permalink to this headline">¶</a></h2>
<p>Goldberger (1991) motivates least squares regression as a technique to estimate a <strong>best-fitting</strong> linear approximation to a conditional expectation function that may be nonlinear in the population.</p>
<p><strong>Best</strong> is defined as minimizing the average squared differences between the fitted values and the true values of the conditional expectations functions.</p>
<p><img alt="126e807adfb944628e17c5b5d403e6fc" class="no-scaled-link" src="../../_images/fig-regression-demonstration-one.png" style="width: 500px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span> <span class="o">=</span> <span class="n">get_sample_demonstration_1</span><span class="p">(</span><span class="n">num_agents</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
<span class="n">df</span><span class="o">.</span><span class="n">head</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Y</th>
      <th>D</th>
      <th>S</th>
      <th>Y_1</th>
      <th>Y_0</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0.113157</td>
      <td>0</td>
      <td>1</td>
      <td>4.055376</td>
      <td>0.113157</td>
    </tr>
    <tr>
      <th>1</th>
      <td>9.479227</td>
      <td>0</td>
      <td>3</td>
      <td>14.146062</td>
      <td>9.479227</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0.409400</td>
      <td>0</td>
      <td>1</td>
      <td>2.081023</td>
      <td>0.409400</td>
    </tr>
    <tr>
      <th>3</th>
      <td>7.087262</td>
      <td>1</td>
      <td>2</td>
      <td>7.087262</td>
      <td>5.145585</td>
    </tr>
    <tr>
      <th>4</th>
      <td>3.338352</td>
      <td>0</td>
      <td>1</td>
      <td>2.825938</td>
      <td>3.338352</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">])[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
D  S
0  1     2.014537
   2     6.032069
   3     9.976885
1  1     4.067050
   2     8.028103
   3    14.025534
Name: Y, dtype: float64
</pre></div></div>
</div>
<p>How does the functional form of the conditional expectation look like?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plot_conditional_expectation_demonstration_1</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_regression-estimators_notebook_10_0.png" src="../../_images/lectures_regression-estimators_notebook_10_0.png" />
</div>
</div>
<p>What does the difference between the two lines tell us about treatment effect heterogeneity?</p>
<p>We will fit four different prediction models using ordinary least squares.</p>
<p><span class="math">\begin{align*}
&\hat{Y} = \beta_0 + \beta_1 D + \beta_2 S \\
&\hat{Y} = \beta_0 + \beta_1 D + \beta_2 S_1 + \beta_3 S_2 \\
&\hat{Y} = \beta_0 + \beta_1 D + \beta_2 S_1 + \beta_3 S_2 + \beta_4 S_1 * D + \beta_5 S_2 * D
\end{align*}</span></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D + S&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">rslt</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.941</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.941</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>8.018e+04</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 26 May 2020</td> <th>  Prob (F-statistic):</th>  <td>  0.00</td>
</tr>
<tr>
  <th>Time:</th>                 <td>07:30:39</td>     <th>  Log-Likelihood:    </th> <td> -15339.</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td> 10000</td>      <th>  AIC:               </th> <td>3.068e+04</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>  9997</td>      <th>  BIC:               </th> <td>3.071e+04</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>     2</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>Intercept</th> <td>   -2.6594</td> <td>    0.027</td> <td>  -98.877</td> <td> 0.000</td> <td>   -2.712</td> <td>   -2.607</td>
</tr>
<tr>
  <th>D</th>         <td>    2.7202</td> <td>    0.025</td> <td>  108.756</td> <td> 0.000</td> <td>    2.671</td> <td>    2.769</td>
</tr>
<tr>
  <th>S</th>         <td>    4.4181</td> <td>    0.014</td> <td>  311.459</td> <td> 0.000</td> <td>    4.390</td> <td>    4.446</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 1.627</td> <th>  Durbin-Watson:     </th> <td>   2.023</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.443</td> <th>  Jarque-Bera (JB):  </th> <td>   1.637</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.016</td> <th>  Prob(JB):          </th> <td>   0.441</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.946</td> <th>  Cond. No.          </th> <td>    6.15</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;predict&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">rslt</span><span class="o">.</span><span class="n">predict</span><span class="p">()</span>
<span class="n">df</span><span class="o">.</span><span class="n">groupby</span><span class="p">([</span><span class="s2">&quot;D&quot;</span><span class="p">,</span> <span class="s2">&quot;S&quot;</span><span class="p">])[[</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;predict&quot;</span><span class="p">]]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th></th>
      <th>Y</th>
      <th>predict</th>
    </tr>
    <tr>
      <th>D</th>
      <th>S</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th rowspan="3" valign="top">0</th>
      <th>1</th>
      <td>2.014537</td>
      <td>1.758657</td>
    </tr>
    <tr>
      <th>2</th>
      <td>6.032069</td>
      <td>6.176733</td>
    </tr>
    <tr>
      <th>3</th>
      <td>9.976885</td>
      <td>10.594808</td>
    </tr>
    <tr>
      <th rowspan="3" valign="top">1</th>
      <th>1</th>
      <td>4.067050</td>
      <td>4.478865</td>
    </tr>
    <tr>
      <th>2</th>
      <td>8.028103</td>
      <td>8.896941</td>
    </tr>
    <tr>
      <th>3</th>
      <td>14.025534</td>
      <td>13.315017</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plot_predictions_demonstration_1</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_regression-estimators_notebook_15_0.png" src="../../_images/lectures_regression-estimators_notebook_15_0.png" />
</div>
</div>
<div class="section" id="Anscombe-quartet">
<h3>Anscombe quartet<a class="headerlink" href="#Anscombe-quartet" title="Permalink to this headline">¶</a></h3>
<p>The best linear approximation can be the same for very different functions. The <strong>Anscombe quartet</strong> (Anscombe, 1973) and many other useful datasets are available in <code class="docutils literal notranslate"><span class="pre">statsmodels</span></code> as part of the <a class="reference external" href="https://www.statsmodels.org/0.8.0/datasets/index.html">Datasets Package</a>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[30]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">df3</span><span class="p">,</span> <span class="n">df4</span> <span class="o">=</span> <span class="n">get_anscombe_datasets</span><span class="p">()</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">df</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">([</span><span class="n">df1</span><span class="p">,</span> <span class="n">df2</span><span class="p">,</span> <span class="n">df3</span><span class="p">,</span> <span class="n">df4</span><span class="p">]):</span>
    <span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;y ~ x&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="s2"> Dataset </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot; Intercept: </span><span class="si">{:5.3f}</span><span class="s2"> x: </span><span class="si">{:5.3f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="o">*</span><span class="n">rslt</span><span class="o">.</span><span class="n">params</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>

 Dataset 0
 Intercept: 3.000 x: 0.500

 Dataset 1
 Intercept: 3.001 x: 0.500

 Dataset 2
 Intercept: 3.002 x: 0.500

 Dataset 3
 Intercept: 3.002 x: 0.500
</pre></div></div>
</div>
<p>So what does the data behind these regressions look like?</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[31]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plot_anscombe_dataset</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_regression-estimators_notebook_20_0.png" src="../../_images/lectures_regression-estimators_notebook_20_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="Regression-adjustment-as-a-strategy-to-estimate-causal-effects">
<h2>Regression adjustment as a strategy to estimate causal effects<a class="headerlink" href="#Regression-adjustment-as-a-strategy-to-estimate-causal-effects" title="Permalink to this headline">¶</a></h2>
<div class="section" id="Regression-models-and-omitted-variable-bias">
<h3>Regression models and omitted-variable bias<a class="headerlink" href="#Regression-models-and-omitted-variable-bias" title="Permalink to this headline">¶</a></h3>
<p><span class="math">\begin{align*}
Y = \alpha + \delta D + \epsilon
\end{align*}</span></p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(\delta\)</span> is interpreted as an invariant, structural causal effect that applies to all members of the population.</p></li>
<li><p><span class="math notranslate nohighlight">\(\epsilon\)</span> is a summary random variable that represents all other causes of <span class="math notranslate nohighlight">\(Y\)</span>.</p></li>
</ul>
<p><span class="math">\begin{align*}
\hat{\delta}_{OLS, \text{bivariate}} = \frac{Cov_N(y_i, d_i)}{Var_N(d_i)}
\end{align*}</span></p>
<p>It now depends on the correlation between <span class="math notranslate nohighlight">\(\epsilon\)</span> and <span class="math notranslate nohighlight">\(D\)</span> whether <span class="math notranslate nohighlight">\(\hat{\delta}\)</span> provides an unbiased and consistent estimate of the true causal effect</p>
<p><img alt="a2aeebec853b4cc28f1e3f222239cd23" class="no-scaled-link" src="../../_images/fig-omitted-variable-bias.png" style="width: 300px;" /></p>
<p>We now move to the potential outcomes model to clarify the connection between <strong>omitted-variable bias</strong> and <strong>self-selection bias</strong>.</p>
</div>
</div>
<div class="section" id="Potential-outcomes-and-omitted-variable-bias">
<h2>Potential outcomes and omitted-variable bias<a class="headerlink" href="#Potential-outcomes-and-omitted-variable-bias" title="Permalink to this headline">¶</a></h2>
<p><span class="math">\begin{align*}
Y = \underbrace{\mu^0}_{\alpha} + \underbrace{(\mu^1 - \mu^0)}_{\delta} D + \underbrace{\{\nu^0 + D(\nu^1 - \nu^0 )\}}_{\epsilon},
\end{align*}</span></p>
<p>where <span class="math notranslate nohighlight">\(\mu^0\equiv E[Y^0]\)</span>, <span class="math notranslate nohighlight">\(\mu^1\equiv E[Y^1]\)</span>, <span class="math notranslate nohighlight">\(\nu^0\equiv Y^0 - E[Y^0]\)</span>, and <span class="math notranslate nohighlight">\(\nu^1\equiv Y^1 - E[Y^1]\)</span>.</p>
<p>What induces a correlation between <span class="math notranslate nohighlight">\(D\)</span> an <span class="math notranslate nohighlight">\(\{\nu^0 + D(\nu^1 - \nu^0 )\}\)</span>?</p>
<ul class="simple">
<li><p><strong>baseline bias</strong>, there is a net baseline difference in the hypothetical no-treatment state that is correlated with treatment uptake <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(D\)</span> is correlated with <span class="math notranslate nohighlight">\(\nu_0\)</span></p></li>
<li><p><strong>differential treatment bias</strong>, there is a net treatment effect difference that is correlated with treatment uptake <span class="math notranslate nohighlight">\(\rightarrow\)</span> <span class="math notranslate nohighlight">\(D\)</span> is correlated with <span class="math notranslate nohighlight">\(D(\nu^1 - \nu^0 )\)</span></p></li>
</ul>
<p><img alt="9424da9de0074fc2a4cfcfdbd97f3c6c" class="no-scaled-link" src="../../_images/fig-regression-demonstration-two.png" style="width: 300px; height: 300px;" /></p>
<p><strong>Errata</strong></p>
<p>Please note that there is a relevant correction on the author’s <a class="reference external" href="http://socweb.soc.jhu.edu/faculty/morgan/papers/Errata_2nd_Edition.pdf">website</a>:</p>
<ul class="simple">
<li><p>page 198, Table 6.2, first panel: In order to restrict the bias to differential baseline bias only, as required by the label on the first panel of the table, replace 20 with 10 in the first cell of the second row.Then, carry the changes across columns so that (a) the values for <span class="math notranslate nohighlight">\(\nu^1_i\)</span> are 5 for the individual in the treatment group and -5 for the individual in the control group and (b) the value for <span class="math notranslate nohighlight">\({\nu^0_i + D(\nu^1_i − \nu^0_i)}\)</span> is 5 for the individual in the treatment
group</p></li>
</ul>
<table class="docutils align-default">
<colgroup>
<col style="width: 6%" />
<col style="width: 5%" />
<col style="width: 6%" />
<col style="width: 8%" />
<col style="width: 8%" />
<col style="width: 4%" />
<col style="width: 4%" />
<col style="width: 33%" />
<col style="width: 25%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Group</p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i^0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\nu_i^1\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\nu_i^0\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(y_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(d_i\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\nu_i^1 + d_i (\nu_i^1 - \nu_i^0)\)</span></p></th>
<th class="head"><p>$ d_i (<span class="math">\nu</span>_i^1 - <span class="math">\nu</span>_i^0)$</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Treated</p></td>
<td><p>20</p></td>
<td><p>10</p></td>
<td><p>5</p></td>
<td><p>5</p></td>
<td><p>20</p></td>
<td><p>1</p></td>
<td><p>5.000000000000000000000000</p></td>
<td><p>0.000000000</p></td>
</tr>
<tr class="row-odd"><td><p>Control</p></td>
<td><p>10</p></td>
<td><p>0</p></td>
<td><p>-5</p></td>
<td><p>-5</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>-5</p></td>
<td><p>0.000000000</p></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="Regression-as-adjustment-for-otherwise-omitted-variables">
<h2>Regression as adjustment for otherwise omitted variables<a class="headerlink" href="#Regression-as-adjustment-for-otherwise-omitted-variables" title="Permalink to this headline">¶</a></h2>
<p><img alt="9e2ee9cac74047e28a497862c82b60e9" class="no-scaled-link" src="../../_images/fig-observable-regression-adjustment.png" style="width: 300px;" /></p>
<p>We first want to illustrate how we can <em>subtract out</em> the dependence between <span class="math notranslate nohighlight">\(D\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> induced by their common determinant <span class="math notranslate nohighlight">\(X\)</span>. Let’s quickly simulate a parameterized example:</p>
<p><span class="math">\begin{align*}
D & = I[X + \eta > 0]  \\
Y & = D + X + \epsilon,
\end{align*}</span> where <span class="math notranslate nohighlight">\((\eta, \epsilon)\)</span> follow a standard normal distribution.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[118]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df</span> <span class="o">=</span> <span class="n">get_quick_sample</span><span class="p">(</span><span class="n">num_samples</span><span class="o">=</span><span class="mi">1000</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>We now first run a complete regression.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[122]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">stat</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D + X&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated effect: </span><span class="si">{</span><span class="n">stat</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Estimated effect: 0.924
</pre></div></div>
</div>
<p>However, as it turns out, we can also get the identical estimate by first partialling out the effect of <span class="math notranslate nohighlight">\(X\)</span> on <span class="math notranslate nohighlight">\(D\)</span> as well as <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[127]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">df_resid</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;Y_resid&quot;</span><span class="p">,</span> <span class="s2">&quot;D_resid&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">,</span> <span class="s2">&quot;D&quot;</span><span class="p">]:</span>
    <span class="n">column</span><span class="p">,</span> <span class="n">formula</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2">_resid&quot;</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">label</span><span class="si">}</span><span class="s2"> ~ X&quot;</span>
    <span class="n">df_resid</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">column</span><span class="p">]</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">resid</span>

<span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y_resid ~ D_resid&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df_resid</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Estimated effect: </span><span class="si">{</span><span class="n">stat</span><span class="si">:</span><span class="s2">5.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Estimated effect: 0.924
</pre></div></div>
</div>
<p>We will now look at two datasets that are observationally equivalent but regression adjustment for observable <span class="math notranslate nohighlight">\(X\)</span> does only work in one of them.</p>
<p><img alt="d3f8ecfca0f04763969c34baf07be547" class="no-scaled-link" src="../../_images/fig-regression-demonstration-four.png" style="width: 500px;" /></p>
<p><strong>Note</strong></p>
<ul class="simple">
<li><p>The naive estimates will be identical as the observed values <span class="math notranslate nohighlight">\(y_i\)</span> and <span class="math notranslate nohighlight">\(d_i\)</span> are the same.</p></li>
</ul>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[40]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">get_sample_regression_adjustment</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample </span><span class="si">{:}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span>

    <span class="n">stat</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Y_1&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Y_0&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;True effect:    </span><span class="si">{:5.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stat</span><span class="p">))</span>

    <span class="n">stat</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;D == 1&quot;</span><span class="p">)[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;D == 0&quot;</span><span class="p">)[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Naive estimate: </span><span class="si">{:5.4f}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">stat</span><span class="p">))</span>

    <span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">rslt</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sample 0

True effect:    10.0000
Naive estimate: 11.6540
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.860
Model:                            OLS   Adj. R-squared:                  0.860
Method:                 Least Squares   F-statistic:                     6113.
Date:                Tue, 26 May 2020   Prob (F-statistic):               0.00
Time:                        11:44:40   Log-Likelihood:                -2275.1
No. Observations:                1000   AIC:                             4554.
Df Residuals:                     998   BIC:                             4564.
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      6.8379      0.105     65.272      0.000       6.632       7.044
D             11.6540      0.149     78.187      0.000      11.361      11.946
==============================================================================
Omnibus:                     7278.240   Durbin-Watson:                   1.966
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               94.873
Skew:                          -0.097   Prob(JB):                     2.50e-21
Kurtosis:                       1.504   Cond. No.                         2.60
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
Sample 1

True effect:    9.6280
Naive estimate: 11.6540
                            OLS Regression Results
==============================================================================
Dep. Variable:                      Y   R-squared:                       0.860
Model:                            OLS   Adj. R-squared:                  0.860
Method:                 Least Squares   F-statistic:                     6113.
Date:                Tue, 26 May 2020   Prob (F-statistic):               0.00
Time:                        11:44:42   Log-Likelihood:                -2275.1
No. Observations:                1000   AIC:                             4554.
Df Residuals:                     998   BIC:                             4564.
Df Model:                           1
Covariance Type:            nonrobust
==============================================================================
                 coef    std err          t      P&gt;|t|      [0.025      0.975]
------------------------------------------------------------------------------
Intercept      6.8379      0.105     65.272      0.000       6.632       7.044
D             11.6540      0.149     78.187      0.000      11.361      11.946
==============================================================================
Omnibus:                     7278.240   Durbin-Watson:                   1.966
Prob(Omnibus):                  0.000   Jarque-Bera (JB):               94.873
Skew:                          -0.097   Prob(JB):                     2.50e-21
Kurtosis:                       1.504   Cond. No.                         2.60
==============================================================================

Warnings:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
</pre></div></div>
</div>
<p>Now we condition on <span class="math notranslate nohighlight">\(X\)</span> to see where conditioning might help in obtaining an unbiased estimate of the true effect. Note that the treatment effect <span class="math notranslate nohighlight">\((y^1_i - y^0_i)\)</span> is uncorrelated with <span class="math notranslate nohighlight">\(d_i\)</span> within each strata of <span class="math notranslate nohighlight">\(X\)</span> in the first example. That is not true in the second example.</p>
<p><img alt="277db3d55e5a4f7e868d1a46589f8d96" class="no-scaled-link" src="../../_images/fig-regression-demonstration-five.png" style="width: 500px;" /></p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[39]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="k">for</span> <span class="n">sample</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">):</span>

    <span class="n">df</span> <span class="o">=</span> <span class="n">get_sample_regression_adjustment</span><span class="p">(</span><span class="n">sample</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Sample </span><span class="si">{:}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">sample</span><span class="p">))</span>

    <span class="n">stat</span> <span class="o">=</span> <span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s2">&quot;Y_1&quot;</span><span class="p">]</span> <span class="o">-</span> <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Y_0&quot;</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;True effect:</span><span class="si">{</span><span class="n">stat</span><span class="si">:</span><span class="s2">24.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">stat</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;D == 1&quot;</span><span class="p">)[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="o">-</span> <span class="n">df</span><span class="o">.</span><span class="n">query</span><span class="p">(</span><span class="s2">&quot;D == 0&quot;</span><span class="p">)[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Naive estimate:</span><span class="si">{</span><span class="n">stat</span><span class="si">:</span><span class="s2">21.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="s2">&quot;Y ~ D + X&quot;</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Conditional estimate:</span><span class="si">{</span><span class="n">rslt</span><span class="o">.</span><span class="n">params</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">15.4f</span><span class="si">}</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Sample 0

True effect:                 10.0000
Naive estimate:              11.6540
Conditional estimate:        10.0000

Sample 1

True effect:                  9.6280
Naive estimate:              11.6540
Conditional estimate:        10.0000

</pre></div></div>
</div>
<p>To summarize: Regression adjustment by <span class="math notranslate nohighlight">\(X\)</span> will yield a consistent and unbiased estimate of the ATE when:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(D\)</span> is mean independent of (and therefore uncorrelated with) <span class="math notranslate nohighlight">\(v^0 + D(v^1 - v^0)\)</span> for each subset of respondent identified by distinct values on the variables in <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>the causal effect of <span class="math notranslate nohighlight">\(D\)</span> does not vary with <span class="math notranslate nohighlight">\(X\)</span></p></li>
<li><p>a fully flexible parameterization of <span class="math notranslate nohighlight">\(X\)</span> is used</p></li>
</ul>
</div>
<div class="section" id="Freedman’s-paradox">
<h2>Freedman’s paradox<a class="headerlink" href="#Freedman’s-paradox" title="Permalink to this headline">¶</a></h2>
<p>Let’s explore some of the challenges of finding the right regression specification.</p>
<blockquote>
<div><p>In statistical analysis, Freedman’s paradox (Freedman, 1983), named after David Freedman, is a problem in model selection whereby predictor variables with no relationship to the dependent variable can pass tests of significance – both individually via a t-test, and jointly via an F-test for the significance of the regression. (Wikipedia)</p>
</div></blockquote>
<p>We fill a dataframe with random numbers. Thus there is no causal relationship between the dependent and independent variables.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[33]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">columns</span> <span class="o">=</span> <span class="p">[</span><span class="s2">&quot;Y&quot;</span><span class="p">]</span>
<span class="p">[</span><span class="n">columns</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="s2">&quot;X</span><span class="si">{:}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">i</span><span class="p">))</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">50</span><span class="p">)]</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">51</span><span class="p">)),</span> <span class="n">columns</span><span class="o">=</span><span class="n">columns</span><span class="p">)</span>
</pre></div>
</div>
</div>
<p>Now we run a simple regression of the random independent variables on the dependent variable.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;Y ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">columns</span><span class="p">[</span><span class="mi">1</span><span class="p">:])</span>
<span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">rslt</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[34]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.545</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.081</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   1.176</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 26 May 2020</td> <th>  Prob (F-statistic):</th>  <td> 0.286</td>
</tr>
<tr>
  <th>Time:</th>                 <td>11:32:24</td>     <th>  Log-Likelihood:    </th> <td> -108.43</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   318.9</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    49</td>      <th>  BIC:               </th> <td>   451.7</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    50</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>Intercept</th> <td>    0.1106</td> <td>    0.136</td> <td>    0.811</td> <td> 0.421</td> <td>   -0.163</td> <td>    0.384</td>
</tr>
<tr>
  <th>X0</th>        <td>   -0.0922</td> <td>    0.222</td> <td>   -0.416</td> <td> 0.679</td> <td>   -0.538</td> <td>    0.353</td>
</tr>
<tr>
  <th>X1</th>        <td>   -0.1713</td> <td>    0.141</td> <td>   -1.217</td> <td> 0.229</td> <td>   -0.454</td> <td>    0.111</td>
</tr>
<tr>
  <th>X2</th>        <td>   -0.0741</td> <td>    0.155</td> <td>   -0.478</td> <td> 0.635</td> <td>   -0.386</td> <td>    0.237</td>
</tr>
<tr>
  <th>X3</th>        <td>    0.1575</td> <td>    0.134</td> <td>    1.178</td> <td> 0.244</td> <td>   -0.111</td> <td>    0.426</td>
</tr>
<tr>
  <th>X4</th>        <td>    0.2736</td> <td>    0.172</td> <td>    1.595</td> <td> 0.117</td> <td>   -0.071</td> <td>    0.618</td>
</tr>
<tr>
  <th>X5</th>        <td>   -0.0649</td> <td>    0.172</td> <td>   -0.378</td> <td> 0.707</td> <td>   -0.410</td> <td>    0.280</td>
</tr>
<tr>
  <th>X6</th>        <td>    0.0063</td> <td>    0.141</td> <td>    0.045</td> <td> 0.964</td> <td>   -0.276</td> <td>    0.289</td>
</tr>
<tr>
  <th>X7</th>        <td>    0.1089</td> <td>    0.141</td> <td>    0.774</td> <td> 0.443</td> <td>   -0.174</td> <td>    0.392</td>
</tr>
<tr>
  <th>X8</th>        <td>    0.0694</td> <td>    0.139</td> <td>    0.500</td> <td> 0.619</td> <td>   -0.210</td> <td>    0.348</td>
</tr>
<tr>
  <th>X9</th>        <td>    0.3075</td> <td>    0.149</td> <td>    2.059</td> <td> 0.045</td> <td>    0.007</td> <td>    0.608</td>
</tr>
<tr>
  <th>X10</th>       <td>    0.0189</td> <td>    0.168</td> <td>    0.113</td> <td> 0.911</td> <td>   -0.318</td> <td>    0.356</td>
</tr>
<tr>
  <th>X11</th>       <td>   -0.2898</td> <td>    0.178</td> <td>   -1.626</td> <td> 0.110</td> <td>   -0.648</td> <td>    0.068</td>
</tr>
<tr>
  <th>X12</th>       <td>    0.0207</td> <td>    0.129</td> <td>    0.160</td> <td> 0.873</td> <td>   -0.239</td> <td>    0.280</td>
</tr>
<tr>
  <th>X13</th>       <td>    0.0901</td> <td>    0.119</td> <td>    0.757</td> <td> 0.453</td> <td>   -0.149</td> <td>    0.329</td>
</tr>
<tr>
  <th>X14</th>       <td>   -0.1296</td> <td>    0.154</td> <td>   -0.843</td> <td> 0.403</td> <td>   -0.439</td> <td>    0.179</td>
</tr>
<tr>
  <th>X15</th>       <td>    0.0618</td> <td>    0.153</td> <td>    0.405</td> <td> 0.687</td> <td>   -0.245</td> <td>    0.368</td>
</tr>
<tr>
  <th>X16</th>       <td>   -0.1151</td> <td>    0.122</td> <td>   -0.944</td> <td> 0.350</td> <td>   -0.360</td> <td>    0.130</td>
</tr>
<tr>
  <th>X17</th>       <td>    0.1265</td> <td>    0.170</td> <td>    0.744</td> <td> 0.460</td> <td>   -0.215</td> <td>    0.468</td>
</tr>
<tr>
  <th>X18</th>       <td>    0.1578</td> <td>    0.140</td> <td>    1.129</td> <td> 0.264</td> <td>   -0.123</td> <td>    0.439</td>
</tr>
<tr>
  <th>X19</th>       <td>   -0.0752</td> <td>    0.157</td> <td>   -0.477</td> <td> 0.635</td> <td>   -0.392</td> <td>    0.241</td>
</tr>
<tr>
  <th>X20</th>       <td>   -0.1454</td> <td>    0.133</td> <td>   -1.097</td> <td> 0.278</td> <td>   -0.412</td> <td>    0.121</td>
</tr>
<tr>
  <th>X21</th>       <td>    0.1507</td> <td>    0.150</td> <td>    1.005</td> <td> 0.320</td> <td>   -0.151</td> <td>    0.452</td>
</tr>
<tr>
  <th>X22</th>       <td>    0.4160</td> <td>    0.133</td> <td>    3.123</td> <td> 0.003</td> <td>    0.148</td> <td>    0.684</td>
</tr>
<tr>
  <th>X23</th>       <td>   -0.1169</td> <td>    0.156</td> <td>   -0.750</td> <td> 0.457</td> <td>   -0.430</td> <td>    0.196</td>
</tr>
<tr>
  <th>X24</th>       <td>   -0.2271</td> <td>    0.165</td> <td>   -1.376</td> <td> 0.175</td> <td>   -0.559</td> <td>    0.104</td>
</tr>
<tr>
  <th>X25</th>       <td>    0.1651</td> <td>    0.172</td> <td>    0.962</td> <td> 0.341</td> <td>   -0.180</td> <td>    0.510</td>
</tr>
<tr>
  <th>X26</th>       <td>    0.1461</td> <td>    0.123</td> <td>    1.192</td> <td> 0.239</td> <td>   -0.100</td> <td>    0.392</td>
</tr>
<tr>
  <th>X27</th>       <td>   -0.2343</td> <td>    0.139</td> <td>   -1.685</td> <td> 0.098</td> <td>   -0.514</td> <td>    0.045</td>
</tr>
<tr>
  <th>X28</th>       <td>    0.0508</td> <td>    0.138</td> <td>    0.368</td> <td> 0.715</td> <td>   -0.227</td> <td>    0.329</td>
</tr>
<tr>
  <th>X29</th>       <td>   -0.0187</td> <td>    0.191</td> <td>   -0.098</td> <td> 0.922</td> <td>   -0.403</td> <td>    0.365</td>
</tr>
<tr>
  <th>X30</th>       <td>    0.2245</td> <td>    0.149</td> <td>    1.511</td> <td> 0.137</td> <td>   -0.074</td> <td>    0.523</td>
</tr>
<tr>
  <th>X31</th>       <td>    0.0353</td> <td>    0.146</td> <td>    0.242</td> <td> 0.810</td> <td>   -0.258</td> <td>    0.329</td>
</tr>
<tr>
  <th>X32</th>       <td>    0.0666</td> <td>    0.152</td> <td>    0.438</td> <td> 0.663</td> <td>   -0.239</td> <td>    0.372</td>
</tr>
<tr>
  <th>X33</th>       <td>   -0.0281</td> <td>    0.151</td> <td>   -0.186</td> <td> 0.853</td> <td>   -0.331</td> <td>    0.275</td>
</tr>
<tr>
  <th>X34</th>       <td>    0.0204</td> <td>    0.141</td> <td>    0.144</td> <td> 0.886</td> <td>   -0.263</td> <td>    0.304</td>
</tr>
<tr>
  <th>X35</th>       <td>   -0.1940</td> <td>    0.138</td> <td>   -1.409</td> <td> 0.165</td> <td>   -0.471</td> <td>    0.083</td>
</tr>
<tr>
  <th>X36</th>       <td>    0.1215</td> <td>    0.144</td> <td>    0.843</td> <td> 0.403</td> <td>   -0.168</td> <td>    0.411</td>
</tr>
<tr>
  <th>X37</th>       <td>    0.3450</td> <td>    0.171</td> <td>    2.023</td> <td> 0.049</td> <td>    0.002</td> <td>    0.688</td>
</tr>
<tr>
  <th>X38</th>       <td>    0.2652</td> <td>    0.148</td> <td>    1.787</td> <td> 0.080</td> <td>   -0.033</td> <td>    0.563</td>
</tr>
<tr>
  <th>X39</th>       <td>    0.0370</td> <td>    0.167</td> <td>    0.221</td> <td> 0.826</td> <td>   -0.299</td> <td>    0.373</td>
</tr>
<tr>
  <th>X40</th>       <td>   -0.0072</td> <td>    0.147</td> <td>   -0.049</td> <td> 0.961</td> <td>   -0.302</td> <td>    0.288</td>
</tr>
<tr>
  <th>X41</th>       <td>    0.2258</td> <td>    0.154</td> <td>    1.469</td> <td> 0.148</td> <td>   -0.083</td> <td>    0.535</td>
</tr>
<tr>
  <th>X42</th>       <td>   -0.1910</td> <td>    0.154</td> <td>   -1.242</td> <td> 0.220</td> <td>   -0.500</td> <td>    0.118</td>
</tr>
<tr>
  <th>X43</th>       <td>    0.1973</td> <td>    0.139</td> <td>    1.423</td> <td> 0.161</td> <td>   -0.081</td> <td>    0.476</td>
</tr>
<tr>
  <th>X44</th>       <td>   -0.1017</td> <td>    0.136</td> <td>   -0.750</td> <td> 0.457</td> <td>   -0.374</td> <td>    0.171</td>
</tr>
<tr>
  <th>X45</th>       <td>   -0.1656</td> <td>    0.137</td> <td>   -1.210</td> <td> 0.232</td> <td>   -0.441</td> <td>    0.109</td>
</tr>
<tr>
  <th>X46</th>       <td>   -0.0255</td> <td>    0.152</td> <td>   -0.168</td> <td> 0.867</td> <td>   -0.330</td> <td>    0.279</td>
</tr>
<tr>
  <th>X47</th>       <td>    0.1621</td> <td>    0.148</td> <td>    1.094</td> <td> 0.279</td> <td>   -0.136</td> <td>    0.460</td>
</tr>
<tr>
  <th>X48</th>       <td>    0.1768</td> <td>    0.144</td> <td>    1.228</td> <td> 0.225</td> <td>   -0.113</td> <td>    0.466</td>
</tr>
<tr>
  <th>X49</th>       <td>    0.1961</td> <td>    0.146</td> <td>    1.345</td> <td> 0.185</td> <td>   -0.097</td> <td>    0.489</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 0.122</td> <th>  Durbin-Watson:     </th> <td>   2.167</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.941</td> <th>  Jarque-Bera (JB):  </th> <td>   0.302</td>
</tr>
<tr>
  <th>Skew:</th>          <td>-0.007</td> <th>  Prob(JB):          </th> <td>   0.860</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.731</td> <th>  Cond. No.          </th> <td>    6.59</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div>
</div>
<p>We use this to inform a second regression where we only keep the variables that were significant at the 25% level.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">final_covariates</span> <span class="o">=</span> <span class="nb">list</span><span class="p">()</span>
<span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">rslt</span><span class="o">.</span><span class="n">params</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">rslt</span><span class="o">.</span><span class="n">pvalues</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="o">&gt;</span> <span class="mf">0.25</span><span class="p">:</span>
        <span class="k">continue</span>
    <span class="n">final_covariates</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">label</span><span class="p">)</span>

<span class="n">formula</span> <span class="o">=</span> <span class="s2">&quot;Y ~ &quot;</span> <span class="o">+</span> <span class="s2">&quot; + &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">final_covariates</span><span class="p">)</span>
<span class="n">rslt</span> <span class="o">=</span> <span class="n">smf</span><span class="o">.</span><span class="n">ols</span><span class="p">(</span><span class="n">formula</span><span class="o">=</span><span class="n">formula</span><span class="p">,</span> <span class="n">data</span><span class="o">=</span><span class="n">df</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">rslt</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<table class="simpletable">
<caption>OLS Regression Results</caption>
<tr>
  <th>Dep. Variable:</th>            <td>Y</td>        <th>  R-squared:         </th> <td>   0.402</td>
</tr>
<tr>
  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.260</td>
</tr>
<tr>
  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   2.834</td>
</tr>
<tr>
  <th>Date:</th>             <td>Tue, 26 May 2020</td> <th>  Prob (F-statistic):</th> <td>0.000627</td>
</tr>
<tr>
  <th>Time:</th>                 <td>07:19:03</td>     <th>  Log-Likelihood:    </th> <td> -122.11</td>
</tr>
<tr>
  <th>No. Observations:</th>      <td>   100</td>      <th>  AIC:               </th> <td>   284.2</td>
</tr>
<tr>
  <th>Df Residuals:</th>          <td>    80</td>      <th>  BIC:               </th> <td>   336.3</td>
</tr>
<tr>
  <th>Df Model:</th>              <td>    19</td>      <th>                     </th>     <td> </td>
</tr>
<tr>
  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>
</tr>
</table>
<table class="simpletable">
<tr>
      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>
</tr>
<tr>
  <th>Intercept</th> <td>    0.0853</td> <td>    0.103</td> <td>    0.832</td> <td> 0.408</td> <td>   -0.119</td> <td>    0.289</td>
</tr>
<tr>
  <th>X1</th>        <td>   -0.0874</td> <td>    0.099</td> <td>   -0.879</td> <td> 0.382</td> <td>   -0.285</td> <td>    0.111</td>
</tr>
<tr>
  <th>X3</th>        <td>    0.1712</td> <td>    0.101</td> <td>    1.698</td> <td> 0.093</td> <td>   -0.029</td> <td>    0.372</td>
</tr>
<tr>
  <th>X4</th>        <td>    0.1905</td> <td>    0.111</td> <td>    1.710</td> <td> 0.091</td> <td>   -0.031</td> <td>    0.412</td>
</tr>
<tr>
  <th>X9</th>        <td>    0.2888</td> <td>    0.104</td> <td>    2.768</td> <td> 0.007</td> <td>    0.081</td> <td>    0.496</td>
</tr>
<tr>
  <th>X11</th>       <td>   -0.2161</td> <td>    0.119</td> <td>   -1.812</td> <td> 0.074</td> <td>   -0.453</td> <td>    0.021</td>
</tr>
<tr>
  <th>X22</th>       <td>    0.3742</td> <td>    0.100</td> <td>    3.735</td> <td> 0.000</td> <td>    0.175</td> <td>    0.574</td>
</tr>
<tr>
  <th>X24</th>       <td>   -0.2796</td> <td>    0.103</td> <td>   -2.716</td> <td> 0.008</td> <td>   -0.484</td> <td>   -0.075</td>
</tr>
<tr>
  <th>X26</th>       <td>    0.1391</td> <td>    0.094</td> <td>    1.484</td> <td> 0.142</td> <td>   -0.047</td> <td>    0.326</td>
</tr>
<tr>
  <th>X27</th>       <td>   -0.2348</td> <td>    0.103</td> <td>   -2.283</td> <td> 0.025</td> <td>   -0.439</td> <td>   -0.030</td>
</tr>
<tr>
  <th>X30</th>       <td>    0.1220</td> <td>    0.110</td> <td>    1.109</td> <td> 0.271</td> <td>   -0.097</td> <td>    0.341</td>
</tr>
<tr>
  <th>X35</th>       <td>   -0.1628</td> <td>    0.093</td> <td>   -1.742</td> <td> 0.085</td> <td>   -0.349</td> <td>    0.023</td>
</tr>
<tr>
  <th>X37</th>       <td>    0.2214</td> <td>    0.100</td> <td>    2.206</td> <td> 0.030</td> <td>    0.022</td> <td>    0.421</td>
</tr>
<tr>
  <th>X38</th>       <td>    0.2400</td> <td>    0.106</td> <td>    2.267</td> <td> 0.026</td> <td>    0.029</td> <td>    0.451</td>
</tr>
<tr>
  <th>X41</th>       <td>    0.0482</td> <td>    0.102</td> <td>    0.471</td> <td> 0.639</td> <td>   -0.155</td> <td>    0.252</td>
</tr>
<tr>
  <th>X42</th>       <td>   -0.1669</td> <td>    0.113</td> <td>   -1.478</td> <td> 0.143</td> <td>   -0.392</td> <td>    0.058</td>
</tr>
<tr>
  <th>X43</th>       <td>    0.1723</td> <td>    0.096</td> <td>    1.786</td> <td> 0.078</td> <td>   -0.020</td> <td>    0.364</td>
</tr>
<tr>
  <th>X45</th>       <td>   -0.1853</td> <td>    0.101</td> <td>   -1.838</td> <td> 0.070</td> <td>   -0.386</td> <td>    0.015</td>
</tr>
<tr>
  <th>X48</th>       <td>    0.1667</td> <td>    0.092</td> <td>    1.811</td> <td> 0.074</td> <td>   -0.016</td> <td>    0.350</td>
</tr>
<tr>
  <th>X49</th>       <td>    0.1491</td> <td>    0.100</td> <td>    1.492</td> <td> 0.140</td> <td>   -0.050</td> <td>    0.348</td>
</tr>
</table>
<table class="simpletable">
<tr>
  <th>Omnibus:</th>       <td> 2.282</td> <th>  Durbin-Watson:     </th> <td>   2.259</td>
</tr>
<tr>
  <th>Prob(Omnibus):</th> <td> 0.319</td> <th>  Jarque-Bera (JB):  </th> <td>   1.718</td>
</tr>
<tr>
  <th>Skew:</th>          <td> 0.137</td> <th>  Prob(JB):          </th> <td>   0.424</td>
</tr>
<tr>
  <th>Kurtosis:</th>      <td> 2.420</td> <th>  Cond. No.          </th> <td>    2.43</td>
</tr>
</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.</div>
</div>
<p>What to make of this exercise?</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">213</span><span class="p">)</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">columns</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;F-statistic&quot;</span><span class="p">,</span> <span class="s2">&quot;Regressors&quot;</span><span class="p">])</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">100</span><span class="p">):</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">run_freedman_exercise</span><span class="p">()</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">&quot;Regressors&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">params</span><span class="p">)</span>
    <span class="n">df</span><span class="p">[</span><span class="s2">&quot;F-statistic&quot;</span><span class="p">]</span><span class="o">.</span><span class="n">loc</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">f_pvalue</span>
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre>
<span></span><span class="n">plot_freedman_exercise</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/lectures_regression-estimators_notebook_54_0.png" src="../../_images/lectures_regression-estimators_notebook_54_0.png" />
</div>
</div>
</div>
<div class="section" id="Resources">
<h2>Resources<a class="headerlink" href="#Resources" title="Permalink to this headline">¶</a></h2>
<ul class="simple">
<li><p><strong>Goldberger, A. S. (1991)</strong>. <a class="reference external" href="https://www.hup.harvard.edu/catalog.php?isbn=9780674175440">A course in econometrics</a>. Cambridge, MA: <em>Harvard University Press</em>.</p></li>
<li><p><strong>F. J. Anscombe (1973)</strong>. <a class="reference external" href="https://www.sjsu.edu/faculty/gerstman/StatPrimer/anscombe1973.pdf">Graphs in Statistical Analysis</a>. <em>The American Statistician</em>, 27, 17–21.</p></li>
<li><p><strong>Freedman, David A.; Freedman, David A. (1983)</strong>. <a class="reference external" href="https://www.jstor.org/stable/2685877?seq=1">A Note on Screening Regression Equations</a>. <em>The American Statistician</em>. 37 (2), 152–155.</p></li>
</ul>
</div>
</div>


           </div>
           
          </div>
          <footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
        <a href="../selection-heterogeneity-graphs/notebook.html" class="btn btn-neutral float-right" title="Self-selection, heterogeneity, and causal graphs" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
        <a href="../matching-estimators/notebook.html" class="btn btn-neutral float-left" title="Matching estimators of causal effects" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>
        &#169; Copyright 2020, Prof. Dr. Philipp Eisenhauer.

    </p>
  </div>
    
    
    
    Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>
        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>