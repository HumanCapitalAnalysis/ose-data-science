{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import pickle as pkl\n",
    "import os\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from auxiliary import get_propensity_scores_matching_demonstration_4\n",
    "from auxiliary import get_sample_matching_demonstration_2\n",
    "from auxiliary import get_sample_matching_demonstration_3\n",
    "from auxiliary import get_sample_matching_demonstration_4\n",
    "from auxiliary import get_sparsity_pattern_by_treatment\n",
    "from auxiliary import get_sparsity_pattern_overall\n",
    "from auxiliary import get_propensity_score_3\n",
    "from auxiliary import plot_propensity_score\n",
    "from auxiliary import get_common_support\n",
    "from auxiliary import get_lalonde_data\n",
    "from auxiliary import get_inv_odds\n",
    "from auxiliary import plot_weights\n",
    "from auxiliary import get_odds\n",
    "\n",
    "np.random.seed(123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5: Matching estimators of causal effects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There exists only one back-door path $D \\leftarrow S \\leftrightarrow X \\rightarrow Y$ and both $S$ nor $X$ are observable. Thus, we have a choice to condition on either one of them.\n",
    "\n",
    "<img src=\"material/fig-conditioning-balance-adjust.png\" height=300 width=300 />\n",
    "\n",
    "* $X$, regression estimator, adjustment-for-other-causes conditioning strategy\n",
    "* $S$, matching estimator, balancing conditioning strategy\n",
    "\n",
    "**Agenda**\n",
    "\n",
    "* matching as conditioning via stratification\n",
    "* matching as weighting\n",
    "* matching as data analysis algorithm\n",
    "\n",
    "**Fundamental concepts**\n",
    "\n",
    "* stratification of data\n",
    "* weighting to achieve balance\n",
    "* propensity scores\n",
    "\n",
    "**Views on matching**\n",
    "\n",
    "* method to form quasi-experimental contrasts by sampling comparable treatment and control cases\n",
    "* nonparametric method of adjustment for treatment assignment patterns\n",
    "\n",
    "**Simulation data**\n",
    "\n",
    "The simulated data is inspired by real-world applications and thus rather complex. Nevertheless, the will serve as examples for several of the upcoming lectures. That is why we will invest some time initially to set up one of them in details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Matching as conditioning via stratification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Individuals within groups determined by $S$ are entirely indistinguishable from each other in all ways except \n",
    "\n",
    "* observed treatment status\n",
    "\n",
    "* differences in potential outcomes that are independent of treatment status\n",
    "\n",
    "More formally, we are able to assert the following **conditional independence assumptions**.\n",
    "\n",
    "\\begin{align*}\n",
    "E[Y^1 \\mid D = 1, S] = E[Y^1 \\mid D = 0, S] \\\\\n",
    "E[Y^0 \\mid D = 1, S] = E[Y^0 \\mid D = 0, S]\n",
    "\\end{align*}\n",
    "\n",
    "implied by ...\n",
    "\n",
    "* treatment assignment is ignorable\n",
    "* selection on observables\n",
    "\n",
    "**ATC**\n",
    "\n",
    "\\begin{align*}\n",
    "E[\\delta \\mid D = 0, S] & = E[Y^1 - Y^0 \\mid D = 0, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 0, S] - E[Y^0 \\mid D = 0, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 1, S] - E[Y^0 \\mid D = 0, S] \\\\\n",
    "                        & = E[Y \\mid D = 1, S] - E[Y \\mid D = 0, S] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "**ATT**\n",
    "\\begin{align*}\n",
    "E[\\delta \\mid D = 1, S] & = E[Y^1 - Y^0 \\mid D = 1, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 1, S] - E[Y^0 \\mid D = 1, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 1, S] - E[Y^0 \\mid D = 0, S] \\\\\n",
    "                        & = E[Y \\mid D = 1, S] - E[Y \\mid D = 0, S] \\\\\n",
    "\\end{align*}\n",
    "\n",
    "<img src=\"material/fig-matching-demonstration-1.png\" height=500 width=500 />\n",
    "\n",
    "**Features**\n",
    "\n",
    "* The gains from treatment participation differ in each stratum and those that have the most to gain are more likely to participate. So unconditional independence between $D$ and $(Y^1, Y^2)$ does not hold.\n",
    "\n",
    "Let's study these idealized conditions for a simulated dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let us see our simulation in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are in the comfortable position to not only compute the naive estimate but also the true average treatment effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What to do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Note that the observed outcomes within each stratum correspond to the average potential outcome \n",
    "within the stratum. We can compute the average treatment effect by looking at the difference within each strata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The ATT and ATC can be computed analogously just by applying the appropriate weights to the strata-specific effect of treatment.\n",
    "\n",
    "More generally.\n",
    "\n",
    "\\begin{align*}\n",
    "\\{E_N [y_i \\mid d_i = 1, s = s_i] - E_N [y_i \\mid d_i = 0, s = s_i]\\} \\\\\n",
    "\\xrightarrow{p} E[Y^1 - Y^0\\mid S = s] = E[\\delta \\mid S = s].\n",
    "\\end{align*}\n",
    "Weighted sums of these stratified estimates can then be taken such as for the unconditional ATE:\n",
    "\\begin{align*}\n",
    "& \\sum_s \\{E_N[y_i \\mid d_i = 1, s_i = s] - E_N[y_i \\mid d_i = 0, s_i = s]\\} \\\\\n",
    "& * {\\Pr}_N[s_i = s] \\xrightarrow{p} E[\\delta]\n",
    "\\end{align*}\n",
    "\n",
    "\n",
    "This examples shows all of the basic principles in matching estimators that we will discuss in \n",
    "greater detail in this lecture. \n",
    "\n",
    "* Treatment and control subjects are matched together in the sense that they are grouped together \n",
    "into strata.\n",
    "\n",
    "* An average difference between the outcomes of the treatment and control subjects is estimated, \n",
    "based on a weighting of the strata by common distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Overlap conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Y</th>\n",
       "      <th>D</th>\n",
       "      <th>S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.220843</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.505549</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.814226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.453293</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.939864</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Y  D  S\n",
       "0   3.220843  0  1\n",
       "1  15.505549  1  3\n",
       "2   0.814226  0  1\n",
       "3   1.453293  0  1\n",
       "4   2.939864  0  1"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = get_sample_matching_demonstration_2(num_agents=1000)\n",
    "df[[\"Y\", \"D\", \"S\"]].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "S  D\n",
       "1  0     1.986109\n",
       "2  0     5.854302\n",
       "   1     8.184356\n",
       "3  0    10.245960\n",
       "   1    13.978689\n",
       "Name: Y, dtype: float64"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"S\", \"D\"])[\"Y\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "What can we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Matching as weighting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As indicated by the stylized example, there are often many strata where we do not have treated and control individuals available at the same time.\n",
    "\n",
    "$\\rightarrow$ combine information from different strata with the same propensity score $p$\n",
    "\n",
    "**Definition** The estimated propensity score is the estimated probability of taking the treatment as a function of variables that predict treatment assignment, i.e. $\\Pr[D = 1 \\mid S]$.\n",
    "\n",
    "$\\rightarrow$ stratifying on the propensity score itself ameliorates the sparseness problem because the propensity score can be treated as a single stratifying variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "a_grid = np.linspace(0.01, 1.00, 100)\n",
    "b_grid = np.linspace(0.01, 1.00, 100)\n",
    "\n",
    "df, counts = get_sample_matching_demonstration_3(a_grid, b_grid)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**underlying causal graphs**\n",
    "\n",
    "<img src=\"material/fig-matching-demonstration-3.png\" height=500 width=500 />\n",
    "\n",
    "We will now look at different ways to construct estimates for the usual causal parameters. So, we first compute their true counterparts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "true_effects = list()\n",
    "true_effects += [(df[\"y_1\"] - df[\"y_0\"])[(df[\"d\"] == 1)].mean()]\n",
    "true_effects += [(df[\"y_1\"] - df[\"y_0\"])[(df[\"d\"] == 0)].mean()]\n",
    "true_effects += [(df[\"y_1\"] - df[\"y_0\"]).mean()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "print(\n",
    "    \"The true estimate of the average causal effect is {:5.3f}\".format(\n",
    "        true_effects[-1]\n",
    "    )\n",
    ")\n",
    "\n",
    "stat = df[\"y\"][df[\"d\"] == 1].mean() - df[\"y\"][df[\"d\"] == 0].mean()\n",
    "print(\n",
    "    \"The naive estimate of the average causal effect is {:5.3f}\".format(stat)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about the issue of sparsity on the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "get_sparsity_pattern_overall(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "get_sparsity_pattern_by_treatment(counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the propensity score $P(D = 1\\mid S)$ as a function of the observables $(a, b)$ look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "plot_propensity_score(a_grid, b_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We still must be worried about common support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "get_common_support(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\delta}_{\\text{ATT, weight}} \\equiv \\left( \\frac{1}{n^1}\\sum_{i:d_i = 1} y_i\\right) \n",
    "- \\left(\\frac{\\sum_{i:d_i=0}\\hat{r}_i y_i}{\\sum_{i:d_i = 0} \\hat{r}_i}\\right)\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\delta}_{\\text{ATC, weight}} \\equiv \n",
    "\\left(\n",
    "\\frac{\\sum_{i: d_i = 1}\\frac{y_i}{\\hat{r}_i}}{\\sum_{i: d_i = 1}\\frac{1}{\\hat{r}_i}}\n",
    "\\right)\n",
    "- \\left(\\frac{1}{n^0} \\sum_{i: d_i = 0} y_i\\right) \n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\delta}_{\\text{ATE, weight}} \\equiv \\left(\\frac{1}{n}\\sum_{i}d_i\\right) \\hat{\\delta}_{\\text{ATT, weight}} +  \\left(1 - \\frac{1}{n}\\sum_{i}d_i\\right) \\hat{\\delta}_{\\text{ATC, weight}}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "\n",
    "**Weights** \n",
    "\n",
    "\\begin{align*}\n",
    "r_i = \\frac{p_i}{1 - p_i}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {},
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# This example is important as it introduces students to the\n",
    "# actual setup for the estimation of the propensity score\n",
    "# and its potential misspecification.\n",
    "def get_att_weight(df, p):\n",
    "    \"\"\" Get weighted ATT.\n",
    "    \n",
    "    Calculates the weighted ATT basd on a provided\n",
    "    dataset and the propensity score.\n",
    "    \n",
    "    Args:\n",
    "        df: A dataframe with the observed data.\n",
    "        p: A numpy array with the weights.\n",
    "        \n",
    "    Returns:\n",
    "        A float which corresponds to the ATT.\n",
    "    \"\"\"\n",
    "    weights = get_odds(p)\n",
    "\n",
    "    is_control = df[\"d\"] == 0\n",
    "    is_treated = df[\"d\"] == 1\n",
    "\n",
    "    value, weights = df[\"y\"][is_control], weights[is_control]\n",
    "    att = df[\"y\"][is_treated].mean() - np.average(value, weights=weights)\n",
    "\n",
    "    return att\n",
    "\n",
    "\n",
    "def get_atc_weight(df, p):\n",
    "    \"\"\" Get weighted ATC.\n",
    "    \n",
    "    Calculates the weighted ATC basd on a provided\n",
    "    dataset and the propensity score.\n",
    "    \n",
    "    Args:\n",
    "        df: A dataframe with the observed data.\n",
    "        p: A numpy array with the weights.\n",
    "        \n",
    "    Returns:\n",
    "        A float which corresponds to the ATC.\n",
    "    \"\"\"\n",
    "    weights = get_inv_odds(p)\n",
    "\n",
    "    is_control = df[\"d\"] == 0\n",
    "    is_treated = df[\"d\"] == 1\n",
    "\n",
    "    value, weights = df[\"y\"][is_treated], weights[is_treated]\n",
    "    atc = np.average(value, weights=weights) - df[\"y\"][is_control].mean()\n",
    "\n",
    "    return atc\n",
    "\n",
    "\n",
    "def get_ate_weight(df, p):\n",
    "    \"\"\" Get weighted ATE.\n",
    "    \n",
    "    Calculates the weighted ATE basd on a provided\n",
    "    dataset and the propensity score.\n",
    "    \n",
    "    Args:\n",
    "        df: A dataframe with the observed data.\n",
    "        p: A numpy array with the weights.\n",
    "        \n",
    "    Returns:\n",
    "        A float which corresponds to the ATE.\n",
    "    \"\"\"\n",
    "    share_treated = df[\"d\"].value_counts(normalize=True)[1]\n",
    "\n",
    "    atc = get_atc_weight(df, p)\n",
    "    att = get_att_weight(df, p)\n",
    "\n",
    "    return share_treated * att + (1.0 - share_treated) * atc\n",
    "\n",
    "\n",
    "rslt = dict()\n",
    "for model in [\"true\", \"correct\", \"misspecified\"]:\n",
    "    p = get_propensity_score_3(df, model)\n",
    "\n",
    "    rslt[model] = list()\n",
    "    rslt[model] += [get_att_weight(df, p)]\n",
    "    rslt[model] += [get_atc_weight(df, p)]\n",
    "    rslt[model] += [get_ate_weight(df, p)]\n",
    "\n",
    "    print(\"\")\n",
    "    print(model.capitalize())\n",
    "    print(\n",
    "        \"estimated: ATT {:5.3f} ATC {:5.3f} ATE {:5.3f}\".format(*rslt[model])\n",
    "    )\n",
    "    print(\n",
    "        \"true:      ATT {:5.3f} ATC {:5.3f} ATE {:5.3f}\".format(*true_effects)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "toc-hr-collapsed": true,
    "toc-nb-collapsed": true
   },
   "source": [
    "## Matching as data analysis algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\\begin{align*}\n",
    "\\hat{\\delta}_{\\text{ATT, match}} = \\frac{1}{n^1} \\sum_i \\left[\n",
    "(y_i \\mid d_i = 1) - \\sum_j \\omega_{i, j} (y_j \\mid d_j =0 )\n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "\\hat{\\delta}_{\\text{ATC, match}} = \\frac{1}{n^0} \\sum_j \\left[\n",
    " \\sum_i \\omega_{j, i} (y_i\\mid d_i = 1) - (y_j \\mid d_j = 0)\n",
    "\\right]\n",
    "\\end{align*}\n",
    "\n",
    "### Basic variants\n",
    "\n",
    "* exact matching\n",
    "\n",
    "    * construct counterfactual based on individuals with identical $S$\n",
    "\n",
    "\n",
    "* nearest-neighbor and caliper\n",
    "\n",
    "    * construct counterfactual based on individuals closest on a unidimensional measure, caliper ensures reasonable maximum distance\n",
    "\n",
    "* interval matching\n",
    "\n",
    "    * construct counterfactual by sorting individuals into segments based on unidimensional metric\n",
    "\n",
    "\n",
    "* kernel matching\n",
    "\n",
    "    * constructs counterfactual based on all individuals but weights them cased on the distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Benchmarking tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = get_sample_matching_demonstration_4()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "example_covariates = [\"black\", \"urban\", \"test\"]\n",
    "df[example_covariates].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.countplot(x=\"treat\", data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-catholic-school-example.png\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Is there any hope in identifying the $ATE$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-matching-demonstration-four-propensity-score.png\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-matching-demonstration-four-potential-outcome.png\" height=500 width=500 />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here comes the key feature that generates the dependence between $D$ and $Y$ based on an unobservable.\n",
    "\n",
    "\\begin{align*}\n",
    "y_i^1 = y_i^0 + \\delta^\\prime_i + \\delta^{\\prime\\prime}_i\n",
    "\\end{align*}\n",
    "\n",
    "$\\rightarrow$ $\\delta^{\\prime\\prime}_i$ is a associated with one of the potential outcomes and also affects the probability to select treatment.\n",
    "\n",
    "However, we can still identify the $ATT$. Why?\n",
    "\n",
    "\\begin{align*}\n",
    "E[\\delta \\mid D = 1, S] & = E[Y^1 - Y^0 \\mid D = 1, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 1, S] - E[Y^0 \\mid D = 1, S] \\\\\n",
    "                        & = E[Y^1 \\mid D = 1, S] - E[Y^0 \\mid D = 0, S] \\\\\n",
    "                        & = E[Y \\mid D = 1, S] - E[Y \\mid D = 0, S] \\\\\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "stat = (df[\"yt\"] - df[\"yc\"])[df[\"treat\"] == 1].mean()\n",
    "print(\"The true ATT is {:5.3f}\".format(stat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def nearest_neighbor_algorithm_for_att(df, check_store=True):\n",
    "\n",
    "    if check_store:\n",
    "        if os.path.exists(\"matched.ngbr.pkl\"):\n",
    "            return pkl.load(open(\"matched.ngbr.pkl\", \"rb\"))\n",
    "\n",
    "    # We select all treated individuals\n",
    "    df_control = df[df[\"treat\"] == 0]\n",
    "    df_treated = df[df[\"treat\"] == 1]\n",
    "\n",
    "    rslt = np.full((df_treated.shape[0], 11), np.nan)\n",
    "\n",
    "    # We now iterate over all treated individuals and\n",
    "    # find a set of neighbors.\n",
    "    for i, (index, row) in enumerate(df_treated.iterrows()):\n",
    "\n",
    "        y, p, b, u, t = row[[\"y\", \"p\", \"black\", \"urban\", \"test\"]]\n",
    "        df_control = df_control.assign(distance=np.abs(df_control[\"p\"] - p))\n",
    "\n",
    "        idx_ngbr = df_control[\"distance\"].idxmin()\n",
    "        y_ngbr, p_ngbr = df_control.loc[idx_ngbr, [\"y\", \"p\"]]\n",
    "        b_ngbr, u_ngbr = df_control.loc[idx_ngbr, [\"black\", \"urban\"]]\n",
    "        t_ngbr = df_control.loc[idx_ngbr, [\"test\"]]\n",
    "\n",
    "        rslt[i] = [i, y, y_ngbr, p, p_ngbr, b, b_ngbr, u, u_ngbr, t, t_ngbr]\n",
    "\n",
    "    columns = [\"count\", \"y\", \"y_ngbr\", \"p\", \"p_ngbr\", \"b\", \"b_ngbr\"]\n",
    "    columns += [\"u\", \"u_ngbr\", \"t\", \"t_ngbr\"]\n",
    "    df = pd.DataFrame(rslt, columns=columns)\n",
    "\n",
    "    pkl.dump(df, open(\"matched.ngbr.pkl\", \"wb\"))\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "df = get_sample_matching_demonstration_4()\n",
    "df[\"p\"] = get_propensity_scores_matching_demonstration_4(df)\n",
    "get_common_support(df, \"treat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df_matched = nearest_neighbor_algorithm_for_att(df, False)\n",
    "sns.jointplot(\"p\", \"p_ngbr\", df_matched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.jointplot(\"y\", \"y_ngbr\", df_matched)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do our covariantes balance across treatment status?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"treat\")[example_covariates].mean().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We now want to revisit the balancing of covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "rename = {\n",
    "    \"y_ngbr\": \"y\",\n",
    "    \"p_ngbr\": \"p\",\n",
    "    \"b_ngbr\": \"b\",\n",
    "    \"u_ngbr\": \"u\",\n",
    "    \"t_ngbr\": \"t\",\n",
    "}\n",
    "\n",
    "df_control = df_matched[[\"y_ngbr\", \"p_ngbr\", \"b_ngbr\", \"u_ngbr\", \"t_ngbr\"]]\n",
    "df_control = df_control.rename(columns=rename)\n",
    "df_control = df_control.assign(treat=0)\n",
    "\n",
    "df_treated = df_matched[[\"y\", \"p\", \"b\", \"u\", \"t\"]]\n",
    "df_treated = df_treated.assign(treat=1)\n",
    "\n",
    "df_subset = pd.concat([df_treated, df_control])\n",
    "df_subset.groupby(\"treat\").mean().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's take a little detour and look at the balancing of observables in the Lalonde dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = get_lalonde_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "example_covariates = [\"black\", \"married\", \"hispanic\", \"re75\"]\n",
    "df.groupby(\"treat\")[example_covariates].mean().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "The covariates are balanced before any reweighting thanks to the assignment mechanisms.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which matching algorihtm is the best?\n",
    "\n",
    "<img src=\"material/fig-matching-demonstration-four-benchmarking.png\" height=500 width=500 />"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
