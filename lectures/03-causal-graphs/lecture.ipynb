{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from auxiliary import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "---\n",
    "# Lecture 3: Causal graphs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Agenda**\n",
    "\n",
    "* Basic elements of causal graphs\n",
    "* Causal graphs and structural equation\n",
    "* Causal graphs and the potential outcome model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Graph notation less general than potential outcome framework, but \n",
    "\n",
    "* thinking about causal systems\n",
    "* uncover identification strategies\n",
    "\n",
    "> It is useful to separate the inferential problem into statistical and identification components. Studies of identification seek to characterize the conclusions that could be drawn if one could use the sampling process to obtain an unlimited number of observations. (Manski, 1995)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The two most crucial ingredients for an identification analysis are:\n",
    "\n",
    "* The set of assumptions about causal relationships that the analysis is willing to assert based on theory and past research, including assumptions about relationships between variables that have not been observed but that are related both to the cause and outcome of interest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* The pattern of information one can assume would be contained in the joint distribution of the variables in the observed dataset if all members of the population had been included in the sample that generated the dataset.\n",
    "\n",
    "$\\rightarrow$ causal graphs offer an effective and efficient representation for both"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Basic elements of causal graphs\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "* nodes\n",
    "* edges\n",
    "* directed paths\n",
    "    * parent and child\n",
    "    * decendent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<img src=\"material/fig-graph-with-cycle.png\" height=\"200\" width=\"200\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Two representations of the joint depdendence of $A$ and $B$ on an unobserved common cause.\n",
    "\n",
    "<img src=\"material/fig-graph-shorthand-unobserved-common-cause.png\" height=\"500\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at some basic patterns that will turn out to appear frequently.\n",
    "\n",
    "* chain of mediation\n",
    "* fork of mutual causation\n",
    "* inverted fork of mutual dependence, **collider variable**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-basic-causal-relationships.png\" height=\"200\" width=\"200\" />\n",
    "\n",
    "What about the unconditional and conditional association of $A$ and $B$ in each of these cases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conditioning and confounding\n",
    "\n",
    "<img src=\"material/fig-confounding-variable.png\" height=\"500\" width=\"500\"/>\n",
    "\n",
    "* $C$ is a **confounding variable** that affects both the dependent and independent variable.\n",
    "\n",
    "* Conditioning is a modelig strategy that allows to determine causal effects in the presence of observed confounders.\n",
    "\n",
    "$\\rightarrow$ What happens if $C$ is unobserved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How about an example from educational choice where we have observed and unobserved confounders?\n",
    "\n",
    "<img src=\"material/fig-confounders-education.png\" height=\"500\" width=\"500\"/>  \n",
    "\n",
    "What identification strategies come to mind?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Graphs and structural equations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's look at another example and assume we are interested in the effect of parental background (P), charter schools (D), and neighborhoods (N) on test scores (Y).\n",
    "\n",
    "We could set up the following **linear** regression equations:\n",
    "\n",
    "\\begin{align*}\n",
    "D & = \\alpha_D + b_P P + \\epsilon_D \\\\\n",
    "Y & = \\alpha_Y + b_D D + b_P P + + b_N N + \\epsilon_Y\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-equivalent-representations-standard.png\" height=\"500\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-equivalent-representations-magnified.png\" height=\"500\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can set up the same *nonparametric* structural equations for both representations:\n",
    "\n",
    "\\begin{align*}\n",
    "P & = f_P(\\epsilon_1)    \\\\\n",
    "N & = f_N(\\epsilon_3) \\\\\n",
    "D & = f_D(P, \\epsilon_2) \\\\\n",
    "Y & = f_Y(P, D, N, \\epsilon_4)\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to simulate a sample from a set of structural equations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c7e79cb30658>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# distributional assumptions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mget_unobservable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mget_observable\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# parametrization of linear equations\n",
    "alpha_D = 1\n",
    "alpha_Y = 1\n",
    "\n",
    "beta_P = 0.8\n",
    "beta_N = 0.7\n",
    "beta_D = -0.3\n",
    "\n",
    "# distributional assumptions\n",
    "get_unobservable = np.random.normal\n",
    "get_observable = np.random.uniform\n",
    "\n",
    "num_agents = 10000\n",
    "data = np.tile(np.nan, (num_agents, 4))\n",
    "for i in range(num_agents):\n",
    "    P = get_observable()\n",
    "    N = get_observable()\n",
    "    D = alpha_D + beta_P * P + get_unobservable()\n",
    "    Y = alpha_Y + beta_D * D + beta_P * P + beta_N * N + get_unobservable()\n",
    "    data[i, :] = [Y, D, P, N]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Y', 'D', 'P', 'N'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now lets see if we can uncover the structural parameters by a simple ordinary-least-squares regression and thus go full circle from a parametric structural equation model to a causal graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-e7e0edec24ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpatsy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mdmatrices\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdmatrices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Y ~ D + P + N'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mmodel_spec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOLS\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmodel_spec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices\n",
    "\n",
    "y, x = dmatrices('Y ~ D + P + N', data = df)\n",
    "model_spec = sm.OLS(y, x)\n",
    "model_spec.fit().summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Graphs and the potential outcome model\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Advantages of the potential outcome model\n",
    "\n",
    "* definition of causal states\n",
    "* individual effects as first principle\n",
    "* decomposition of sources of inconsistency\n",
    "* ...\n",
    "\n",
    "However, it is hard to manage the notion for larger causal systems with many confounding variables and treatments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=\"material/fig-observed-confounding-variable.png\" height=\"500\" width=\"500\"/>\n",
    "\n",
    "Based on our previous discussion, unfortunately, $E[Y_1 - Y_0] \\neq E[Y\\mid D = 1] - E[Y\\mid D = 1]$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How can we define the treatment effects from the potential outcome model in here?\n",
    "\n",
    "> Interventions and counterfactuals are defined through a mathematical operator called $do(\\cdot)$, which simulates physical interventions by deleting certain functions from the model, replacing them with a constant. (Pearl, 2012)\n",
    "\n",
    "\\begin{align*}\n",
    "E[Y_1 - Y_0 ]\\quad\\text{corresponds to}\\quad E[Y \\mid do(D=1)] - E[Y \\mid do(D=0)]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Graphical presentation of $do(\\cdot)$ operator\n",
    "\n",
    "<img src=\"material/fig-mutilated-graph.png\" height=\"500\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The $do(\\cdot)$ operator induces a key distinction between the **conditional distribution** of the endogenous variable and its **interventional distribution**.\n",
    "\n",
    "Let's simulate a sample from a parametrized version of the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "\n",
    "num_agents = 1000\n",
    "data = np.tile(np.nan, (num_agents, 3))\n",
    "\n",
    "def calculate_outcome(C, D):\n",
    "    \"\"\"We compute the observed outcome.\"\"\"\n",
    "    Y = D + C\n",
    "    \n",
    "    # If you would like to have it in potential\n",
    "    # outcome notation.\n",
    "    Y_1 = 1 + C\n",
    "    Y_0 = 0 + C\n",
    "    Y = D * Y_1 + (1 - D) * Y_1\n",
    "    \n",
    "    # So what is the individual treatment effect?\n",
    "    delta = Y_1 - Y_0\n",
    "        \n",
    "    return D + C \n",
    "    \n",
    "\n",
    "for i in range(num_agents):\n",
    "    C = np.random.uniform()\n",
    "    D = np.random.choice([0, 1], p=[C, 1 - C])\n",
    "    Y = calculate_outcome(C, D)\n",
    "    data[i, :] = [Y, D, C]\n",
    "\n",
    "df = pd.DataFrame(data, columns=['Y', 'D', 'C'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We know how to compute and plot a conditional distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_conditional_distribution(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How can we compute the interventional distribution? What do we need to know to do that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Y_do_1, Y_do_0 = list(), list()\n",
    "for i, row in df.iterrows():\n",
    "    # Note that we calculate the outcome using the \n",
    "    # individual\"s actual C put simply set D to\n",
    "    # its value unter the intervetion.\n",
    "    C, D = row['C'], 1\n",
    "    Y_do_1 += [calculate_outcome(C, D)]\n",
    "\n",
    "    C, D = row['C'], 0            \n",
    "    Y_do_0 += [calculate_outcome(C, D)]\n",
    "\n",
    "plot_interventional_distribution(Y_do_1, Y_do_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# We are now able to compute the causal effect of D on Y.\n",
    "stat = np.mean(Y_do_1) - np.mean(Y_do_0)\n",
    "print('We estimate a causal effect of {:10.5f}'.format(stat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Resources\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Manski, C. F. (1995)**. *Identification problems in the social sciences*. Cambridge, UK: Harvard University Press.  \n",
    "\n",
    "\n",
    "* **Pearl, J. (2012)**. The do-calculus revisited. *arXiv preprint arXiv:1210.4852*."
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
